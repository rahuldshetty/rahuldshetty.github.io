{"type":"data","nodes":[null,{"type":"data","data":[{"post":1},{"slug":2,"date":3,"date_formatted":4,"title":5,"description":6,"metadata":7,"file":66,"content":67},"bringing-van-gogh's-paintings-to-life-with-ai","2024-02-12T00:00:00+05:30","Feb 12, 2024","Bringing Van Gogh's Paintings to Life with AI","Explore how I utilized AI Diffusion models to bring Van Gogh's paintings to life in this blogpost.",{"title":5,"date":3,"draft":8,"description":6,"author":9,"showToc":10,"cover":11,"tags":12,"readingTime":16,"toc":17},"false","Me",true,"https://i.ibb.co/MZnYKQv/00009-3661287321.png",[13,14,15],"Tutorial","AI/ML","GenAI","7",[18,23,27,31,34,38,42,46,50,54,58,62],{"content":19,"slug":20,"lvl":21,"i":22,"seen":22},"Stable Diffusion Art Generation with Web UI","stable-diffusion-art-generation-with-web-ui",1,0,{"content":24,"slug":25,"lvl":26,"i":21,"seen":22},"Web UI Basics","web-ui-basics",2,{"content":28,"slug":29,"lvl":30,"i":26,"seen":22},"Output Demo 1","output-demo-1",3,{"content":32,"slug":33,"lvl":26,"i":30,"seen":22},"Animating frames with AnimateDiff","animating-frames-with-animatediff",{"content":35,"slug":36,"lvl":30,"i":37,"seen":22},"Setup - AnimateDiff Extension","setup---animatediff-extension",4,{"content":39,"slug":40,"lvl":30,"i":41,"seen":22},"Usage - Generating Video","usage---generating-video",5,{"content":43,"slug":44,"lvl":30,"i":45,"seen":22},"Output Demo 2","output-demo-2",6,{"content":47,"slug":48,"lvl":21,"i":49,"seen":22},"Conclusion","conclusion",7,{"content":51,"slug":52,"lvl":21,"i":53,"seen":22},"Software/Tools","software/tools",8,{"content":55,"slug":56,"lvl":26,"i":57,"seen":22},"Diffusion GUI","diffusion-gui",9,{"content":59,"slug":60,"lvl":26,"i":61,"seen":22},"Web UI Extensions","web-ui-extensions",10,{"content":63,"slug":64,"lvl":26,"i":65,"seen":22},"Additional Links","additional-links",11,"2024\\recreating-van-gogh-art-with-ai.md","\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">I posted this video on my \u003Ca href=\"https://www.youtube.com/@AnonymousD3vil\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">YouTube channel\u003C/a>, and it received more views than my typical clips. The inspiration for this video came from the works of the renowned Dutch painter Vincent Van Gogh. In the video, I bring motion and life into his paintings using AI. You can watch the clip below.\u003C/p>\n\u003Ciframe width=\"640\" height=\"400\" src=\"https://www.youtube.com/embed/yntoe0i6QxY?si=d1HHTIGIF_yBXaUs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Alright, when I mentioned that it was generated by AI, it wasn&#39;t entirely accurate. The audio was sourced from the \u003Ca href=\"https://www.youtube.com/@NoCopyrightSounds\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">NCS\u003C/a> channel, and I utilized \u003Ca href=\"https://kdenlive.org/en/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">kdenlive\u003C/a>, another excellent open-source software for advanced video editing, to handle the editing of clips, audio, and text. Nevertheless, I wanted to highlight the capability of some the advancements in AI that can be leveraged to create creative content with ease. \u003C/p>\n\u003Ch1 id=\"stable-diffusion-art-generation-with-web-ui\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Stable Diffusion Art Generation with Web UI\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#stable-diffusion-art-generation-with-web-ui\">#\u003C/a>\u003C/h1>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Before jumping directly into video animation, let us start with the basics, i.e., \u003Cstrong>Image Generation\u003C/strong>. Image generation has advanced since the introduction of \u003Ca href=\"https://en.wikipedia.org/wiki/Diffusion_model\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Diffusion\u003C/a> models. These are currently the true state-of-the-art (SOTA) models for image generation tasks. If you have a \u003Ca href=\"https://www.tomshardware.com/pc-components/gpus/stable-diffusion-benchmarks\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">decent\u003C/a> GPU with over 4GB VRAM, then you can easily run these models on your system.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">To begin, I will use \u003Ca href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">web-ui\u003C/a> by AUTOMATIC111, a beginner-friendly UI offering numerous advanced options for executing image generation tasks with Diffusion models.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/screenshot.png\" alt=\"Web-UI\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">You can follow the \u003Ca href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui?tab=readme-ov-file#installation-on-windows-1011-with-nvidia-gpus-using-release-package\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">instructions\u003C/a> to setup the repository in your local machine.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Download one of the model checkpoints and save it in the repository under the \u003Ccode>models/Stable-diffusion/\u003C/code> location. Refresh the Web UI checkpoint list to find your saved model and select it to load the model into GPU memory.\u003C/p>\n\u003Ctable class=\"max-w-8 divide-y divide-gray-200 dark:divide-gray-700\">\n\u003Cthead class=\"bg-gray-50 dark:bg-gray-700\">\n\u003Ctr>\n\u003Cth class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">Model\u003Cbr>\u003C/th class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">\n\u003Cth class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">Size (GB)\u003C/th class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">\n\u003Cth class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">Link\u003C/th class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">\n\u003C/tr>\n\u003C/thead>\n\u003Ctbody class=\"divide-y divide-gray-200 dark:divide-gray-700\">\u003Ctr>\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">Stable Diffusion v1.5\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">2.13\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\u003Ca href=\"https://huggingface.co/fp16-guy/Stable-Diffusion-v1-5_fp16_cleaned/blob/main/sd_1.5.safetensors\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">https://huggingface.co/fp16-guy/Stable-Diffusion-v1-5_fp16_cleaned/blob/main/sd_1.5.safetensors\u003C/a>\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003C/tr>\n\u003Ctr>\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">SD Turbo\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">5.21\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\u003Ca href=\"https://huggingface.co/stabilityai/sd-turbo/blob/main/sd_turbo.safetensors\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">https://huggingface.co/stabilityai/sd-turbo/blob/main/sd_turbo.safetensors\u003C/a>\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003C/tr>\n\u003Ctr>\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">SDXL Turbo\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">6.94\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\u003Ca href=\"https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors\u003C/a>\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003C/tr>\n\u003Ctr>\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">SD v2.1\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">5.21\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\u003Ca href=\"https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors\u003C/a>\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003C/tr>\n\u003C/tbody>\u003C/table>\n\u003Ch2 id=\"web-ui-basics\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Web UI Basics\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#web-ui-basics\">#\u003C/a>\u003C/h2>\n\u003Cimg src=\"https://i.postimg.cc/prsHCtRM/Screenshot-2024-02-12-113018.png\" alt=\"Web UI Setup\" width=\"900\" height=\"2000\"> \n\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">We will not go over all the features of the Web UI application (that is for another blog post), but I&#39;ll cover some of the basics that will give you an idea of how to get started.\u003C/p>\n\u003Col class=\" space-y-1 list-decimal list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Checkpoint\u003C/strong>: This is where you can choose the base diffusion model.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Positive Prompt\u003C/strong>: This text field is where you will type in the prompt that will be used to generate the image. You&#39;d want to describe the image in terms of quality, content, and all other positive factors that you want to see in the image. In my example, I&#39;ve chosen a simple prompt to create a masterpiece painting of \u003Cem>sunflowers\u003C/em> in the style of the painter \u003Cstrong>Vincent Van Gogh.\u003C/strong>\u003C/p>\n\u003Cdiv class=\"code-block\">\u003Cpre>\u003Ccode>masterpiece, best quality,\nsunflowers, painting,\nart by Vincent Van Gogh, \u003C/code>\u003C/pre>\u003C/div>\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Negative Prompt\u003C/strong>: This is another text field where you can prompt the model on what it should NOT generate. This is good way to guide model to avoid generating bad quality images or to avoid generating a specific subject. In my example, I&#39;ve showm some basic negative prompt that is most oftenly used to avoid bad image quality.\u003C/p>\n\u003Cdiv class=\"code-block\">\u003Cpre>\u003Ccode>bad prompt, bad quality, worst quality,\u003C/code>\u003C/pre>\u003C/div>\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Sampling Method\u003C/strong>: Diffusion model generate the final image by going through intermediate steps where it tries to generate a random image in the latent space and by iteratively predicting and removing the noise from this image. The complete denoising process is termed as \u003Cstrong>Sampling\u003C/strong> and the Sampling Method refers to the process that guides in selecting the random image. There is no best or worst in sampling methods. With lots of trials and experiments you&#39;d need to find the right one for your use-case.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\"> Here are some random sampling methods and its generations. \n \u003Cimg src=\"https://www.andreszsogon.com/wp-content/uploads/xyz_grid-0014-257664911.png\" alt=\"Different Sampling Methods\">\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Sampling Steps\u003C/strong>: In Diffusion models, you&#39;d typically need to define how much steps the model has to take for denoising and refinement. This is called as Sampling Steps. The less number of steps, the faster will be your generation but again there is a little trade-off with the image quality. There are latest techniques like \u003Cem>LCM\u003C/em> or using specific sampling methods like \u003Cem>UniPC\u003C/em> that will allow you to generate a decent looking image with less number of time-steps.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Hires Fix\u003C/strong>: This is used to scale-up your output to larger resolution using UpScalar models. Generating larger images require large VRAM, so these Hires fix will come in handy where you can generate a smaller resolution image (e.g 512x512) then using an Upscalaer method you can increase it.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Output Resolution\u003C/strong>: This is the base resolution to which the diffusion model will generate the initial output. Increasing the resolution will put more load on your GPU. Always use Hires Fix to upscale your images. \u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>CFG Scale\u003C/strong>: Stands for \u003Cem>Classifier-free Guidance\u003C/em>, is a parameter to control how much should the model follow the Text Prompts. In another words, you can say that it controls the creativity of the model. Lower CFG means, the model is more flexible to follow what it thinks is right and higher score puts a weight on it to follow the text prompt. This value also depends on your sampling method as some of the methods like UniPC/LCM does not require a large CFG score to generate output.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Generate\u003C/strong>: You can select this option to start the image diffusion process. The model will use your selected configuration to generate the output. You can also set a predifined \u003Cstrong>Seed\u003C/strong> to control the randomness in the output generation.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Preview\u003C/strong>: Finally the resultant image will be available for preview in this section. It also gets stored under the \u003Ccode>output\u003C/code> directory.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">As per the above configuration shown in the previous image, the final image generation for the Van Gogh&#39;s sunflower painting looks something like this below.\u003C/p>\n\u003Ch3 id=\"output-demo-1\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Output Demo 1\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#output-demo-1\">#\u003C/a>\u003C/h3>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://i.postimg.cc/fbHW5XKr/00000-1765854059.png\" alt=\"Output Generation\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The image resembles Van Gogh&#39;s painting style and also it was able to capture the yellow tints from his sunflower painting. \u003C/p>\n\u003Ch2 id=\"animating-frames-with-animatediff\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Animating frames with AnimateDiff\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#animating-frames-with-animatediff\">#\u003C/a>\u003C/h2>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">This \u003Ca href=\"https://animatediff.github.io/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">paper\u003C/a> introduced very first kind of text-to-video prompting using Stable Diffusion. They prosed a framework that leveraged these \u003Cstrong>motion models\u003C/strong> to animate frames from existing diffusion model output thus generating video. At the time of writing this blog, there is a new and more enhanced text-to-video models called \u003Ca href=\"https://stability.ai/news/stable-video-diffusion-open-ai-video-model\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Stable Video Diffusion\u003C/a> which is from the same developers as Stable Diffusion. You can check them out as well. But for the purpose of this blogpost, I&#39;ll be covering on how to generate video frames using AnimateDiff.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Here is a sample of AnimateDiff.\u003C/p>\n\u003Cvideo width=\"640\" height=\"640\" controls>\n  \u003Csource src=\"https://animatediff.github.io/animations/teaser/ani_03.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n\u003Ch3 id=\"setup---animatediff-extension\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Setup - AnimateDiff Extension\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#setup---animatediff-extension\">#\u003C/a>\u003C/h3>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Another factor contributing to the popularity of Web UI project is their extension support. Within the Open Source community, a plethora of plugins and extension support exists that adds advanced capabilities. One such extension that integrates AnimeDiff into Web UI is \u003Ca href=\"https://github.com/continue-revolution/sd-webui-animatediff\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">sd-webui-animatediff\u003C/a>. To incorporate this extension into your local Web UI setup, follow this \u003Ca href=\"https://github.com/continue-revolution/sd-webui-animatediff?tab=readme-ov-file#how-to-use\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">guide\u003C/a>. Download the required motion modules from the provided links, and you&#39;ll be ready to start using it.\u003C/p>\n\u003Ch3 id=\"usage---generating-video\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Usage - Generating Video\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#usage---generating-video\">#\u003C/a>\u003C/h3>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Navigate to the AnimateDiff configuration window, where you have the option to choose the Frames Per Second (FPS), total number of frames, and looping preferences for the animation frames. Additionally, you can specify the format for saving the video.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://i.ibb.co/LZPKccb/Screenshot-2024-02-12-135242.png\" alt=\"AnimateDiff Configuration\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Once you have chosen the necessary configuration and motion models, proceed to generate the video using your initial text prompt. It&#39;s important to note that the output generation time will be longer compared to image generation. This is expected due to the higher number of frames (images) involved in the generation process and the need to maintain contextual coherence across these frames.\u003C/p>\n\u003Ch3 id=\"output-demo-2\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Output Demo 2\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#output-demo-2\">#\u003C/a>\u003C/h3>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://i.ibb.co/4PKz3rz/00000-3467508380.gif\" alt=\"Sunflower Animation\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">This is how you can animate still images using AnimateDiff. The quality of generation is not superior but, it can be improved with lots of fine-tuning with some tweaks.\u003C/p>\n\u003Ch1 id=\"conclusion\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Conclusion\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#conclusion\">#\u003C/a>\u003C/h1>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Finally, I would like to conclude by stating that we are on the verge of witnessing a tremendous shift in the digital content we consume online. From advertisements to cinema experiences in movie theaters, everything will incorporate some form of these Generative AI applications featuring unreal elements in their content. It is essential for us get acquainted with these technologies as they are will create new opportunities across various sectors.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">While this technology streamlines certain aspects of content creation in digital media, it also opens the door for individuals with malicious intent to utilize it negatively. The prevalence of DeepFakes is on the rise, fraudsters can easily clone voices for deceptive purposes, and verifying the authenticity of simple news content can become even more challenging.\u003C/p>\n\u003Ch1 id=\"software/tools\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Software/Tools\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#software/tools\">#\u003C/a>\u003C/h1>\n\u003Ch2 id=\"diffusion-gui\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Diffusion GUI\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#diffusion-gui\">#\u003C/a>\u003C/h2>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">SD Web UI\u003C/a>: Used in this tutorial\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/comfyanonymous/ComfyUI\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">ComfyUI\u003C/a>: Advanced diffusion GUI tool which takes graphical node based approach to design simple to complex workflows.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"web-ui-extensions\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Web UI Extensions\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#web-ui-extensions\">#\u003C/a>\u003C/h2>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/continue-revolution/sd-webui-animatediff\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">AnimateDiff\u003C/a>: Enable motion model based animation support in webui.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/Mikubill/sd-webui-controlnet\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">ControlNet\u003C/a>: Let&#39;s you to copy human pose, expression, colors, contents, etc of some reference image to your target image.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/CiaraStrawberry/TemporalKit\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">TemporalKit\u003C/a>: Temporal Kit is a helper extension for using EbSynth, an app that lets you style videos frame by frame.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"additional-links\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Additional Links\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#additional-links\">#\u003C/a>\u003C/h2>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://kdenlive.org/en/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Kdenlive\u003C/a>: Advanced video editing software. Completely open source and free to use.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://ncs.io/music\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">NCS\u003C/a>: No Copyrightright Sound for uploading on YouTube/Twitch platforms.\u003C/li>\n\u003C/ul>\n"],"uses":{"params":["slug"]}}]}
