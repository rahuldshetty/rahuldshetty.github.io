<script>
	// It's best to inline this in `head` to avoid FOUC (flash of unstyled content) when changing pages or themes
	if (
	  localStorage.getItem('color-theme') === 'dark' ||
	  (!('color-theme' in localStorage) &&
		window.matchMedia('(prefers-color-scheme: dark)').matches)
	) {
	  document.documentElement.classList.add('dark');
	} else {
	  document.documentElement.classList.remove('dark');
	}
  </script>

<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../../favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../../_app/immutable/assets/0.ab550426.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/fa.95b16411.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/4.e956f1fb.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.ff4a89b7.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/scheduler.97c8b5c4.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/singletons.8d6f1fce.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.f08e7f04.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/paths.06bbe744.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.b29e0e1c.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/preload-helper.a4192956.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.9ebb7127.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.66f5a943.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/each.e59479a4.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/stores.4b8dde1c.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/fa.2329cfa5.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.e870d26e.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/4.75b35614.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/cover.2889fc35.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.7a3d2515.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.73062db0.js"><title>Bringing Van Gogh's Paintings to Life with AI</title><!-- HEAD_svelte-fgg0v1_START --><script async src="https://www.googletagmanager.com/gtag/js?id=G-WVR6SWQH0Z" data-svelte-h="svelte-1thcm6j"></script><!-- HEAD_svelte-fgg0v1_END --><!-- HEAD_svelte-1n6hdy6_START --><meta property="og:site_name" content="RDS's Blog"><meta property="og:title" content="Bringing Van Gogh's Paintings to Life with AI"><meta property="og:type" content="article"><meta property="og:description" content="Explore how I utilized AI Diffusion models to bring Van Gogh's paintings to life in this blogpost."><meta property="og:image" content="https://i.ibb.co/MZnYKQv/00009-3661287321.png"><meta property="article:published_time" content="2024-02-12T00:00:00+05:30"><meta name="twitter:card" content="summary_large_image"><!-- HEAD_svelte-1n6hdy6_END -->
	</head>
	<body data-sveltekit-preload-data="hover" 
		class="dark:bg-[#121212] dark:border-gray-50 flex flex-col min-h-screen transition-colors duration-500"
	>
		<div style="display: contents">   <nav class="dark:bg-[#121212] transition-colors duration-0"><div class="mx-auto max-w-7xl px-2 sm:px-6 lg:px-8"><div class="relative flex h-16 items-center justify-between"><div class="absolute inset-y-0 left-0 flex items-center sm:hidden"> <button type="button" class="relative inline-flex items-center justify-center rounded-md p-2 text-gray-400 focus:outline-none" aria-controls="mobile-menu" aria-expanded="false" data-svelte-h="svelte-1pv4qi8"><span class="absolute -inset-0.5"></span> <span class="sr-only">Open Menu</span>  <svg class="block h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg>  <svg class="hidden h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true"><path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12"></path></svg></button></div> <div class="flex flex-1 items-center justify-center sm:items-stretch sm:justify-start"><div class="flex flex-shrink-0 items-center" data-svelte-h="svelte-frp9wu"><a href="/"><img class="h-8 w-auto" src="/_app/immutable/assets/logo.33e33a6c.png" alt="Website Logo"></a></div> <div class="hidden sm:ml-6 sm:block"> <div class="flex space-x-4"> <a href="/" target="_self" class="text-gray-400 dark:text-[#b3b3b3] hover:bg-gray-700 hover:text-white rounded-md px-3 py-2 text-sm font-medium" aria-current="page"><div class="flex flex-row items-center"> Portfolio</div> </a> <a href="/blog/" target="_self" class="text-gray-700 dark:text-[#b3b3b3] underline underline-offset-4 decoration-2 dark:text-[#b3b3b3] hover:bg-gray-700 hover:text-white rounded-md px-3 py-2 text-sm font-medium" aria-current="page"><div class="flex flex-row items-center"> Blog</div> </a> <a href="/gallery/" target="_self" class="text-gray-400 dark:text-[#b3b3b3] hover:bg-gray-700 hover:text-white rounded-md px-3 py-2 text-sm font-medium" aria-current="page"><div class="flex flex-row items-center"> Gallery</div> </a> <a href="/_app/immutable/assets/resume.51e97c55.pdf" target="_blank" class="text-gray-400 dark:text-[#b3b3b3] hover:bg-gray-700 hover:text-white rounded-md px-3 py-2 text-sm font-medium" aria-current="page"><div class="flex flex-row items-center"><svg class="svelte-fa mr-2 svelte-1cj2gr0" style="height:1em;font-size:.75em;vertical-align:-.125em;transform-origin:center;overflow:visible" viewBox="0 0 512 512" aria-hidden="true" role="img" xmlns="http://www.w3.org/2000/svg"><g transform="translate(256 256)" transform-origin="128 0"><g transform="translate(0,0) scale(1,1)"><path d="M352 0c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9L370.7 96 201.4 265.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L416 141.3l41.4 41.4c9.2 9.2 22.9 11.9 34.9 6.9s19.8-16.6 19.8-29.6V32c0-17.7-14.3-32-32-32H352zM80 32C35.8 32 0 67.8 0 112V432c0 44.2 35.8 80 80 80H400c44.2 0 80-35.8 80-80V320c0-17.7-14.3-32-32-32s-32 14.3-32 32V432c0 8.8-7.2 16-16 16H80c-8.8 0-16-7.2-16-16V112c0-8.8 7.2-16 16-16H192c17.7 0 32-14.3 32-32s-14.3-32-32-32H80z" fill="currentColor" transform="translate(-256 -256)"></path></g></g></svg> Resume</div> </a>  <button id="theme-toggle" type="button" class="text-gray-500 dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:ring-gray-200 dark:focus:ring-gray-700 rounded-full text-sm p-2.5"><svg id="theme-toggle-dark-icon" class="svelte-fa hidden svelte-1cj2gr0" style="height:1em;vertical-align:-.125em;transform-origin:center;overflow:visible" viewBox="0 0 384 512" aria-hidden="true" role="img" xmlns="http://www.w3.org/2000/svg"><g transform="translate(192 256)" transform-origin="96 0"><g transform="translate(0,0) scale(1,1)"><path d="M223.5 32C100 32 0 132.3 0 256S100 480 223.5 480c60.6 0 115.5-24.2 155.8-63.4c5-4.9 6.3-12.5 3.1-18.7s-10.1-9.7-17-8.5c-9.8 1.7-19.8 2.6-30.1 2.6c-96.9 0-175.5-78.8-175.5-176c0-65.8 36-123.1 89.3-153.3c6.1-3.5 9.2-10.5 7.7-17.3s-7.3-11.9-14.3-12.5c-6.3-.5-12.6-.8-19-.8z" fill="gray" transform="translate(-192 -256)"></path></g></g></svg> <svg id="theme-toggle-light-icon" class="svelte-fa hidden svelte-1cj2gr0" style="height:1em;vertical-align:-.125em;transform-origin:center;overflow:visible" viewBox="0 0 512 512" aria-hidden="true" role="img" xmlns="http://www.w3.org/2000/svg"><g transform="translate(256 256)" transform-origin="128 0"><g transform="translate(0,0) scale(1,1)"><path d="M361.5 1.2c5 2.1 8.6 6.6 9.6 11.9L391 121l107.9 19.8c5.3 1 9.8 4.6 11.9 9.6s1.5 10.7-1.6 15.2L446.9 256l62.3 90.3c3.1 4.5 3.7 10.2 1.6 15.2s-6.6 8.6-11.9 9.6L391 391 371.1 498.9c-1 5.3-4.6 9.8-9.6 11.9s-10.7 1.5-15.2-1.6L256 446.9l-90.3 62.3c-4.5 3.1-10.2 3.7-15.2 1.6s-8.6-6.6-9.6-11.9L121 391 13.1 371.1c-5.3-1-9.8-4.6-11.9-9.6s-1.5-10.7 1.6-15.2L65.1 256 2.8 165.7c-3.1-4.5-3.7-10.2-1.6-15.2s6.6-8.6 11.9-9.6L121 121 140.9 13.1c1-5.3 4.6-9.8 9.6-11.9s10.7-1.5 15.2 1.6L256 65.1 346.3 2.8c4.5-3.1 10.2-3.7 15.2-1.6zM160 256a96 96 0 1 1 192 0 96 96 0 1 1 -192 0zm224 0a128 128 0 1 0 -256 0 128 128 0 1 0 256 0z" fill="yellow" transform="translate(-256 -256)"></path></g></g></svg></button></div></div></div></div></div>  <div class="sm:hidden hidden" id="mobile-menu"><div class="space-y-1 px-2 pb-3 pt-2"><a href="/" target="_self" class="text-gray-300 hover:bg-gray-700 hover:text-white block rounded-md px-3 py-2 text-base font-medium text-gray-400"><div class="flex flex-row items-center"> Portfolio</div> </a><a href="/blog/" target="_self" class="text-gray-300 hover:bg-gray-700 hover:text-white block rounded-md px-3 py-2 text-base font-medium text-gray-700"><div class="flex flex-row items-center"> Blog</div> </a><a href="/gallery/" target="_self" class="text-gray-300 hover:bg-gray-700 hover:text-white block rounded-md px-3 py-2 text-base font-medium text-gray-400"><div class="flex flex-row items-center"> Gallery</div> </a><a href="/_app/immutable/assets/resume.51e97c55.pdf" target="_blank" class="text-gray-300 hover:bg-gray-700 hover:text-white block rounded-md px-3 py-2 text-base font-medium text-gray-400"><div class="flex flex-row items-center"><svg class="svelte-fa mr-2 svelte-1cj2gr0" style="height:1em;font-size:.75em;vertical-align:-.125em;transform-origin:center;overflow:visible" viewBox="0 0 512 512" aria-hidden="true" role="img" xmlns="http://www.w3.org/2000/svg"><g transform="translate(256 256)" transform-origin="128 0"><g transform="translate(0,0) scale(1,1)"><path d="M352 0c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9L370.7 96 201.4 265.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L416 141.3l41.4 41.4c9.2 9.2 22.9 11.9 34.9 6.9s19.8-16.6 19.8-29.6V32c0-17.7-14.3-32-32-32H352zM80 32C35.8 32 0 67.8 0 112V432c0 44.2 35.8 80 80 80H400c44.2 0 80-35.8 80-80V320c0-17.7-14.3-32-32-32s-32 14.3-32 32V432c0 8.8-7.2 16-16 16H80c-8.8 0-16-7.2-16-16V112c0-8.8 7.2-16 16-16H192c17.7 0 32-14.3 32-32s-14.3-32-32-32H80z" fill="currentColor" transform="translate(-256 -256)"></path></g></g></svg> Resume</div> </a></div></div></nav> <div class="container mx-auto m-4 overscroll-none justify-center items-center flex relative"> <div class="flex flex-col max-w-2xl max-[600px]:p-10"><h1 class="text-4xl font-bold text-gray-900 place-self-start dark:text-neutral-100">Bringing Van Gogh's Paintings to Life with AI</h1> <p class="text-xl my-1 text-neutral-500">Explore how I utilized AI Diffusion models to bring Van Gogh's paintings to life in this blogpost.</p> <div class="flex flex-row"><span class="inline-flex items-center rounded-md bg-blue-50 font-medium text-blue-600 dark:bg-blue-50 dark:text-neutral-800 px-2 py-1 mr-1 my-1 text-xs ring-1 ring-inset ring-gray-500/10">Tutorial</span><span class="inline-flex items-center rounded-md bg-blue-50 font-medium text-blue-600 dark:bg-blue-50 dark:text-neutral-800 px-2 py-1 mr-1 my-1 text-xs ring-1 ring-inset ring-gray-500/10">AI/ML</span><span class="inline-flex items-center rounded-md bg-blue-50 font-medium text-blue-600 dark:bg-blue-50 dark:text-neutral-800 px-2 py-1 mr-1 my-1 text-xs ring-1 ring-inset ring-gray-500/10">GenAI</span></div> <div class="flex flex-row"><small class="text-neutral-500 dark:text-neutral-600 my-1">Feb 12, 2024</small> <span class="text-neutral-700 dark:text-neutral-700 mx-1" data-svelte-h="svelte-10lih0p">·</span> <small class="text-neutral-500 dark:text-neutral-600 my-1">7 mins</small></div> <div class="flex flex-col place-items-center"><img class="object-cover rounded-lg py-1" src="https://i.ibb.co/MZnYKQv/00009-3661287321.png"> </div> <div class=""><!-- HTML_TAG_START --><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">I posted this video on my <a href="https://www.youtube.com/@AnonymousD3vil" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">YouTube channel</a>, and it received more views than my typical clips. The inspiration for this video came from the works of the renowned Dutch painter Vincent Van Gogh. In the video, I bring motion and life into his paintings using AI. You can watch the clip below.</p>
<iframe width="640" height="400" src="https://www.youtube.com/embed/yntoe0i6QxY?si=d1HHTIGIF_yBXaUs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Alright, when I mentioned that it was generated by AI, it wasn&#39;t entirely accurate. The audio was sourced from the <a href="https://www.youtube.com/@NoCopyrightSounds" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">NCS</a> channel, and I utilized <a href="https://kdenlive.org/en/" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">kdenlive</a>, another excellent open-source software for advanced video editing, to handle the editing of clips, audio, and text. Nevertheless, I wanted to highlight the capability of some the advancements in AI that can be leveraged to create creative content with ease. </p>
<h1 id="stable-diffusion-art-generation-with-web-ui" class="group text-3xl font-bold mt-4 dark:text-neutral-200">Stable Diffusion Art Generation with Web UI<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#stable-diffusion-art-generation-with-web-ui">#</a></h1>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Before jumping directly into video animation, let us start with the basics, i.e., <strong>Image Generation</strong>. Image generation has advanced since the introduction of <a href="https://en.wikipedia.org/wiki/Diffusion_model" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">Diffusion</a> models. These are currently the true state-of-the-art (SOTA) models for image generation tasks. If you have a <a href="https://www.tomshardware.com/pc-components/gpus/stable-diffusion-benchmarks" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">decent</a> GPU with over 4GB VRAM, then you can easily run these models on your system.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">To begin, I will use <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">web-ui</a> by AUTOMATIC111, a beginner-friendly UI offering numerous advanced options for executing image generation tasks with Diffusion models.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><img src="https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/screenshot.png" alt="Web-UI"></p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">You can follow the <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui?tab=readme-ov-file#installation-on-windows-1011-with-nvidia-gpus-using-release-package" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">instructions</a> to setup the repository in your local machine.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Download one of the model checkpoints and save it in the repository under the <code>models/Stable-diffusion/</code> location. Refresh the Web UI checkpoint list to find your saved model and select it to load the model into GPU memory.</p>
<table class="max-w-8 divide-y divide-gray-200 dark:divide-gray-700">
<thead class="bg-gray-50 dark:bg-gray-700">
<tr>
<th class="px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400">Model<br></th class="px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400">
<th class="px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400">Size (GB)</th class="px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400">
<th class="px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400">Link</th class="px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400">
</tr>
</thead>
<tbody class="divide-y divide-gray-200 dark:divide-gray-700"><tr>
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">Stable Diffusion v1.5</td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">2.13</td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200"><a href="https://huggingface.co/fp16-guy/Stable-Diffusion-v1-5_fp16_cleaned/blob/main/sd_1.5.safetensors" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">https://huggingface.co/fp16-guy/Stable-Diffusion-v1-5_fp16_cleaned/blob/main/sd_1.5.safetensors</a></td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
</tr>
<tr>
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">SD Turbo</td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">5.21</td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200"><a href="https://huggingface.co/stabilityai/sd-turbo/blob/main/sd_turbo.safetensors" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">https://huggingface.co/stabilityai/sd-turbo/blob/main/sd_turbo.safetensors</a></td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
</tr>
<tr>
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">SDXL Turbo</td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">6.94</td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200"><a href="https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors</a></td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
</tr>
<tr>
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">SD v2.1</td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">5.21</td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
<td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200"><a href="https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors</a></td class="px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200">
</tr>
</tbody></table>
<h2 id="web-ui-basics" class="group text-2xl font-bold  mt-4 dark:text-neutral-200">Web UI Basics<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#web-ui-basics">#</a></h2>
<img src="https://i.postimg.cc/prsHCtRM/Screenshot-2024-02-12-113018.png" alt="Web UI Setup" width="900" height="2000"> 

<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">We will not go over all the features of the Web UI application (that is for another blog post), but I&#39;ll cover some of the basics that will give you an idea of how to get started.</p>
<ol class=" space-y-1 list-decimal list-inside" >
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Checkpoint</strong>: This is where you can choose the base diffusion model.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Positive Prompt</strong>: This text field is where you will type in the prompt that will be used to generate the image. You&#39;d want to describe the image in terms of quality, content, and all other positive factors that you want to see in the image. In my example, I&#39;ve chosen a simple prompt to create a masterpiece painting of <em>sunflowers</em> in the style of the painter <strong>Vincent Van Gogh.</strong></p>
<div class="code-block"><pre><code>masterpiece, best quality,
sunflowers, painting,
art by Vincent Van Gogh, </code></pre></div></li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Negative Prompt</strong>: This is another text field where you can prompt the model on what it should NOT generate. This is good way to guide model to avoid generating bad quality images or to avoid generating a specific subject. In my example, I&#39;ve showm some basic negative prompt that is most oftenly used to avoid bad image quality.</p>
<div class="code-block"><pre><code>bad prompt, bad quality, worst quality,</code></pre></div></li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Sampling Method</strong>: Diffusion model generate the final image by going through intermediate steps where it tries to generate a random image in the latent space and by iteratively predicting and removing the noise from this image. The complete denoising process is termed as <strong>Sampling</strong> and the Sampling Method refers to the process that guides in selecting the random image. There is no best or worst in sampling methods. With lots of trials and experiments you&#39;d need to find the right one for your use-case.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"> Here are some random sampling methods and its generations. 
 <img src="https://www.andreszsogon.com/wp-content/uploads/xyz_grid-0014-257664911.png" alt="Different Sampling Methods"></p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Sampling Steps</strong>: In Diffusion models, you&#39;d typically need to define how much steps the model has to take for denoising and refinement. This is called as Sampling Steps. The less number of steps, the faster will be your generation but again there is a little trade-off with the image quality. There are latest techniques like <em>LCM</em> or using specific sampling methods like <em>UniPC</em> that will allow you to generate a decent looking image with less number of time-steps.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Hires Fix</strong>: This is used to scale-up your output to larger resolution using UpScalar models. Generating larger images require large VRAM, so these Hires fix will come in handy where you can generate a smaller resolution image (e.g 512x512) then using an Upscalaer method you can increase it.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Output Resolution</strong>: This is the base resolution to which the diffusion model will generate the initial output. Increasing the resolution will put more load on your GPU. Always use Hires Fix to upscale your images. </p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>CFG Scale</strong>: Stands for <em>Classifier-free Guidance</em>, is a parameter to control how much should the model follow the Text Prompts. In another words, you can say that it controls the creativity of the model. Lower CFG means, the model is more flexible to follow what it thinks is right and higher score puts a weight on it to follow the text prompt. This value also depends on your sampling method as some of the methods like UniPC/LCM does not require a large CFG score to generate output.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Generate</strong>: You can select this option to start the image diffusion process. The model will use your selected configuration to generate the output. You can also set a predifined <strong>Seed</strong> to control the randomness in the output generation.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Preview</strong>: Finally the resultant image will be available for preview in this section. It also gets stored under the <code>output</code> directory.</p>
</li>
</ol>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">As per the above configuration shown in the previous image, the final image generation for the Van Gogh&#39;s sunflower painting looks something like this below.</p>
<h3 id="output-demo-1" class="group text-xl font-bold  mt-4 dark:text-neutral-200">Output Demo 1<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#output-demo-1">#</a></h3>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><img src="https://i.postimg.cc/fbHW5XKr/00000-1765854059.png" alt="Output Generation"></p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">The image resembles Van Gogh&#39;s painting style and also it was able to capture the yellow tints from his sunflower painting. </p>
<h2 id="animating-frames-with-animatediff" class="group text-2xl font-bold  mt-4 dark:text-neutral-200">Animating frames with AnimateDiff<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#animating-frames-with-animatediff">#</a></h2>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">This <a href="https://animatediff.github.io/" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">paper</a> introduced very first kind of text-to-video prompting using Stable Diffusion. They prosed a framework that leveraged these <strong>motion models</strong> to animate frames from existing diffusion model output thus generating video. At the time of writing this blog, there is a new and more enhanced text-to-video models called <a href="https://stability.ai/news/stable-video-diffusion-open-ai-video-model" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">Stable Video Diffusion</a> which is from the same developers as Stable Diffusion. You can check them out as well. But for the purpose of this blogpost, I&#39;ll be covering on how to generate video frames using AnimateDiff.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Here is a sample of AnimateDiff.</p>
<video width="640" height="640" controls>
  <source src="https://animatediff.github.io/animations/teaser/ani_03.mp4" type="video/mp4">
</video>

<h3 id="setup---animatediff-extension" class="group text-xl font-bold  mt-4 dark:text-neutral-200">Setup - AnimateDiff Extension<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#setup---animatediff-extension">#</a></h3>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Another factor contributing to the popularity of Web UI project is their extension support. Within the Open Source community, a plethora of plugins and extension support exists that adds advanced capabilities. One such extension that integrates AnimeDiff into Web UI is <a href="https://github.com/continue-revolution/sd-webui-animatediff" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">sd-webui-animatediff</a>. To incorporate this extension into your local Web UI setup, follow this <a href="https://github.com/continue-revolution/sd-webui-animatediff?tab=readme-ov-file#how-to-use" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">guide</a>. Download the required motion modules from the provided links, and you&#39;ll be ready to start using it.</p>
<h3 id="usage---generating-video" class="group text-xl font-bold  mt-4 dark:text-neutral-200">Usage - Generating Video<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#usage---generating-video">#</a></h3>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Navigate to the AnimateDiff configuration window, where you have the option to choose the Frames Per Second (FPS), total number of frames, and looping preferences for the animation frames. Additionally, you can specify the format for saving the video.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><img src="https://i.ibb.co/LZPKccb/Screenshot-2024-02-12-135242.png" alt="AnimateDiff Configuration"></p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Once you have chosen the necessary configuration and motion models, proceed to generate the video using your initial text prompt. It&#39;s important to note that the output generation time will be longer compared to image generation. This is expected due to the higher number of frames (images) involved in the generation process and the need to maintain contextual coherence across these frames.</p>
<h3 id="output-demo-2" class="group text-xl font-bold  mt-4 dark:text-neutral-200">Output Demo 2<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#output-demo-2">#</a></h3>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><img src="https://i.ibb.co/4PKz3rz/00000-3467508380.gif" alt="Sunflower Animation"></p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">This is how you can animate still images using AnimateDiff. The quality of generation is not superior but, it can be improved with lots of fine-tuning with some tweaks.</p>
<h1 id="conclusion" class="group text-3xl font-bold mt-4 dark:text-neutral-200">Conclusion<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#conclusion">#</a></h1>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Finally, I would like to conclude by stating that we are on the verge of witnessing a tremendous shift in the digital content we consume online. From advertisements to cinema experiences in movie theaters, everything will incorporate some form of these Generative AI applications featuring unreal elements in their content. It is essential for us get acquainted with these technologies as they are will create new opportunities across various sectors.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">While this technology streamlines certain aspects of content creation in digital media, it also opens the door for individuals with malicious intent to utilize it negatively. The prevalence of DeepFakes is on the rise, fraudsters can easily clone voices for deceptive purposes, and verifying the authenticity of simple news content can become even more challenging.</p>
<h1 id="software/tools" class="group text-3xl font-bold mt-4 dark:text-neutral-200">Software/Tools<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#software/tools">#</a></h1>
<h2 id="diffusion-gui" class="group text-2xl font-bold  mt-4 dark:text-neutral-200">Diffusion GUI<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#diffusion-gui">#</a></h2>
<ul class=" space-y-1 list-disc list-inside" >
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">SD Web UI</a>: Used in this tutorial</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://github.com/comfyanonymous/ComfyUI" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">ComfyUI</a>: Advanced diffusion GUI tool which takes graphical node based approach to design simple to complex workflows.</li>
</ul>
<h2 id="web-ui-extensions" class="group text-2xl font-bold  mt-4 dark:text-neutral-200">Web UI Extensions<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#web-ui-extensions">#</a></h2>
<ul class=" space-y-1 list-disc list-inside" >
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://github.com/continue-revolution/sd-webui-animatediff" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">AnimateDiff</a>: Enable motion model based animation support in webui.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://github.com/Mikubill/sd-webui-controlnet" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">ControlNet</a>: Let&#39;s you to copy human pose, expression, colors, contents, etc of some reference image to your target image.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://github.com/CiaraStrawberry/TemporalKit" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">TemporalKit</a>: Temporal Kit is a helper extension for using EbSynth, an app that lets you style videos frame by frame.</li>
</ul>
<h2 id="additional-links" class="group text-2xl font-bold  mt-4 dark:text-neutral-200">Additional Links<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#additional-links">#</a></h2>
<ul class=" space-y-1 list-disc list-inside" >
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://kdenlive.org/en/" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">Kdenlive</a>: Advanced video editing software. Completely open source and free to use.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://ncs.io/music" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">NCS</a>: No Copyrightright Sound for uploading on YouTube/Twitch platforms.</li>
</ul>
<!-- HTML_TAG_END --></div></div>    <div class="fixed right-3/4 w-64 max-[600px]:hidden" style="top: 15%;"><a href="#stable-diffusion-art-generation-with-web-ui"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-2">Stable Diffusion Art Generation with Web UI</h4> </a><a href="#web-ui-basics"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-8">Web UI Basics</h4> </a><a href="#output-demo-1"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-16">Output Demo 1</h4> </a><a href="#animating-frames-with-animatediff"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-8">Animating frames with AnimateDiff</h4> </a><a href="#setup---animatediff-extension"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-16">Setup - AnimateDiff Extension</h4> </a><a href="#usage---generating-video"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-16">Usage - Generating Video</h4> </a><a href="#output-demo-2"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-16">Output Demo 2</h4> </a><a href="#conclusion"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-2">Conclusion</h4> </a><a href="#software/tools"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-2">Software/Tools</h4> </a><a href="#diffusion-gui"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-8">Diffusion GUI</h4> </a><a href="#web-ui-extensions"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-8">Web UI Extensions</h4> </a><a href="#additional-links"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-8">Additional Links</h4> </a></div></div> <footer class="text-center lg:text-left mt-auto"><div class="p-4 text-xs text-center text-neutral-400 dark:text-neutral-200">© 2024 <a class="text-xs dark:text-neutral-200 text-neutral-400 underline underline-offset-4" href="/" data-svelte-h="svelte-1h1d2v0">rds</a></div></footer> 
			
			<script>
				{
					__sveltekit_14rqgl5 = {
						base: new URL("../..", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,{"type":"data","data":{post:{slug:"bringing-van-gogh's-paintings-to-life-with-ai",date:"2024-02-12T00:00:00+05:30",date_formatted:"Feb 12, 2024",title:"Bringing Van Gogh's Paintings to Life with AI",description:"Explore how I utilized AI Diffusion models to bring Van Gogh's paintings to life in this blogpost.",metadata:{title:"Bringing Van Gogh's Paintings to Life with AI",date:"2024-02-12T00:00:00+05:30",draft:"false",description:"Explore how I utilized AI Diffusion models to bring Van Gogh's paintings to life in this blogpost.",author:"Me",showToc:true,cover:"https://i.ibb.co/MZnYKQv/00009-3661287321.png",tags:["Tutorial","AI/ML","GenAI"],readingTime:"7",toc:[{content:"Stable Diffusion Art Generation with Web UI",slug:"stable-diffusion-art-generation-with-web-ui",lvl:1,i:0,seen:0},{content:"Web UI Basics",slug:"web-ui-basics",lvl:2,i:1,seen:0},{content:"Output Demo 1",slug:"output-demo-1",lvl:3,i:2,seen:0},{content:"Animating frames with AnimateDiff",slug:"animating-frames-with-animatediff",lvl:2,i:3,seen:0},{content:"Setup - AnimateDiff Extension",slug:"setup---animatediff-extension",lvl:3,i:4,seen:0},{content:"Usage - Generating Video",slug:"usage---generating-video",lvl:3,i:5,seen:0},{content:"Output Demo 2",slug:"output-demo-2",lvl:3,i:6,seen:0},{content:"Conclusion",slug:"conclusion",lvl:1,i:7,seen:0},{content:"Software/Tools",slug:"software/tools",lvl:1,i:8,seen:0},{content:"Diffusion GUI",slug:"diffusion-gui",lvl:2,i:9,seen:0},{content:"Web UI Extensions",slug:"web-ui-extensions",lvl:2,i:10,seen:0},{content:"Additional Links",slug:"additional-links",lvl:2,i:11,seen:0}]},file:"2024\\recreating-van-gogh-art-with-ai.md",content:"\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">I posted this video on my \u003Ca href=\"https://www.youtube.com/@AnonymousD3vil\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">YouTube channel\u003C/a>, and it received more views than my typical clips. The inspiration for this video came from the works of the renowned Dutch painter Vincent Van Gogh. In the video, I bring motion and life into his paintings using AI. You can watch the clip below.\u003C/p>\n\u003Ciframe width=\"640\" height=\"400\" src=\"https://www.youtube.com/embed/yntoe0i6QxY?si=d1HHTIGIF_yBXaUs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Alright, when I mentioned that it was generated by AI, it wasn&#39;t entirely accurate. The audio was sourced from the \u003Ca href=\"https://www.youtube.com/@NoCopyrightSounds\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">NCS\u003C/a> channel, and I utilized \u003Ca href=\"https://kdenlive.org/en/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">kdenlive\u003C/a>, another excellent open-source software for advanced video editing, to handle the editing of clips, audio, and text. Nevertheless, I wanted to highlight the capability of some the advancements in AI that can be leveraged to create creative content with ease. \u003C/p>\n\u003Ch1 id=\"stable-diffusion-art-generation-with-web-ui\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Stable Diffusion Art Generation with Web UI\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#stable-diffusion-art-generation-with-web-ui\">#\u003C/a>\u003C/h1>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Before jumping directly into video animation, let us start with the basics, i.e., \u003Cstrong>Image Generation\u003C/strong>. Image generation has advanced since the introduction of \u003Ca href=\"https://en.wikipedia.org/wiki/Diffusion_model\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Diffusion\u003C/a> models. These are currently the true state-of-the-art (SOTA) models for image generation tasks. If you have a \u003Ca href=\"https://www.tomshardware.com/pc-components/gpus/stable-diffusion-benchmarks\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">decent\u003C/a> GPU with over 4GB VRAM, then you can easily run these models on your system.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">To begin, I will use \u003Ca href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">web-ui\u003C/a> by AUTOMATIC111, a beginner-friendly UI offering numerous advanced options for executing image generation tasks with Diffusion models.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/screenshot.png\" alt=\"Web-UI\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">You can follow the \u003Ca href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui?tab=readme-ov-file#installation-on-windows-1011-with-nvidia-gpus-using-release-package\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">instructions\u003C/a> to setup the repository in your local machine.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Download one of the model checkpoints and save it in the repository under the \u003Ccode>models/Stable-diffusion/\u003C/code> location. Refresh the Web UI checkpoint list to find your saved model and select it to load the model into GPU memory.\u003C/p>\n\u003Ctable class=\"max-w-8 divide-y divide-gray-200 dark:divide-gray-700\">\n\u003Cthead class=\"bg-gray-50 dark:bg-gray-700\">\n\u003Ctr>\n\u003Cth class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">Model\u003Cbr>\u003C/th class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">\n\u003Cth class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">Size (GB)\u003C/th class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">\n\u003Cth class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">Link\u003C/th class=\"px-6 py-3 text-start text-xs font-medium text-gray-500 uppercase dark:text-gray-400\">\n\u003C/tr>\n\u003C/thead>\n\u003Ctbody class=\"divide-y divide-gray-200 dark:divide-gray-700\">\u003Ctr>\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">Stable Diffusion v1.5\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">2.13\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\u003Ca href=\"https://huggingface.co/fp16-guy/Stable-Diffusion-v1-5_fp16_cleaned/blob/main/sd_1.5.safetensors\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">https://huggingface.co/fp16-guy/Stable-Diffusion-v1-5_fp16_cleaned/blob/main/sd_1.5.safetensors\u003C/a>\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003C/tr>\n\u003Ctr>\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">SD Turbo\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">5.21\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\u003Ca href=\"https://huggingface.co/stabilityai/sd-turbo/blob/main/sd_turbo.safetensors\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">https://huggingface.co/stabilityai/sd-turbo/blob/main/sd_turbo.safetensors\u003C/a>\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003C/tr>\n\u003Ctr>\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">SDXL Turbo\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">6.94\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\u003Ca href=\"https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors\u003C/a>\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003C/tr>\n\u003Ctr>\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">SD v2.1\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">5.21\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003Ctd class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\u003Ca href=\"https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors\u003C/a>\u003C/td class=\"px-6 py-4 whitespace-pre-line text-sm font-medium text-gray-800 dark:text-gray-200\">\n\u003C/tr>\n\u003C/tbody>\u003C/table>\n\u003Ch2 id=\"web-ui-basics\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Web UI Basics\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#web-ui-basics\">#\u003C/a>\u003C/h2>\n\u003Cimg src=\"https://i.postimg.cc/prsHCtRM/Screenshot-2024-02-12-113018.png\" alt=\"Web UI Setup\" width=\"900\" height=\"2000\"> \n\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">We will not go over all the features of the Web UI application (that is for another blog post), but I&#39;ll cover some of the basics that will give you an idea of how to get started.\u003C/p>\n\u003Col class=\" space-y-1 list-decimal list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Checkpoint\u003C/strong>: This is where you can choose the base diffusion model.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Positive Prompt\u003C/strong>: This text field is where you will type in the prompt that will be used to generate the image. You&#39;d want to describe the image in terms of quality, content, and all other positive factors that you want to see in the image. In my example, I&#39;ve chosen a simple prompt to create a masterpiece painting of \u003Cem>sunflowers\u003C/em> in the style of the painter \u003Cstrong>Vincent Van Gogh.\u003C/strong>\u003C/p>\n\u003Cdiv class=\"code-block\">\u003Cpre>\u003Ccode>masterpiece, best quality,\nsunflowers, painting,\nart by Vincent Van Gogh, \u003C/code>\u003C/pre>\u003C/div>\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Negative Prompt\u003C/strong>: This is another text field where you can prompt the model on what it should NOT generate. This is good way to guide model to avoid generating bad quality images or to avoid generating a specific subject. In my example, I&#39;ve showm some basic negative prompt that is most oftenly used to avoid bad image quality.\u003C/p>\n\u003Cdiv class=\"code-block\">\u003Cpre>\u003Ccode>bad prompt, bad quality, worst quality,\u003C/code>\u003C/pre>\u003C/div>\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Sampling Method\u003C/strong>: Diffusion model generate the final image by going through intermediate steps where it tries to generate a random image in the latent space and by iteratively predicting and removing the noise from this image. The complete denoising process is termed as \u003Cstrong>Sampling\u003C/strong> and the Sampling Method refers to the process that guides in selecting the random image. There is no best or worst in sampling methods. With lots of trials and experiments you&#39;d need to find the right one for your use-case.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\"> Here are some random sampling methods and its generations. \n \u003Cimg src=\"https://www.andreszsogon.com/wp-content/uploads/xyz_grid-0014-257664911.png\" alt=\"Different Sampling Methods\">\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Sampling Steps\u003C/strong>: In Diffusion models, you&#39;d typically need to define how much steps the model has to take for denoising and refinement. This is called as Sampling Steps. The less number of steps, the faster will be your generation but again there is a little trade-off with the image quality. There are latest techniques like \u003Cem>LCM\u003C/em> or using specific sampling methods like \u003Cem>UniPC\u003C/em> that will allow you to generate a decent looking image with less number of time-steps.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Hires Fix\u003C/strong>: This is used to scale-up your output to larger resolution using UpScalar models. Generating larger images require large VRAM, so these Hires fix will come in handy where you can generate a smaller resolution image (e.g 512x512) then using an Upscalaer method you can increase it.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Output Resolution\u003C/strong>: This is the base resolution to which the diffusion model will generate the initial output. Increasing the resolution will put more load on your GPU. Always use Hires Fix to upscale your images. \u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>CFG Scale\u003C/strong>: Stands for \u003Cem>Classifier-free Guidance\u003C/em>, is a parameter to control how much should the model follow the Text Prompts. In another words, you can say that it controls the creativity of the model. Lower CFG means, the model is more flexible to follow what it thinks is right and higher score puts a weight on it to follow the text prompt. This value also depends on your sampling method as some of the methods like UniPC/LCM does not require a large CFG score to generate output.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Generate\u003C/strong>: You can select this option to start the image diffusion process. The model will use your selected configuration to generate the output. You can also set a predifined \u003Cstrong>Seed\u003C/strong> to control the randomness in the output generation.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Preview\u003C/strong>: Finally the resultant image will be available for preview in this section. It also gets stored under the \u003Ccode>output\u003C/code> directory.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">As per the above configuration shown in the previous image, the final image generation for the Van Gogh&#39;s sunflower painting looks something like this below.\u003C/p>\n\u003Ch3 id=\"output-demo-1\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Output Demo 1\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#output-demo-1\">#\u003C/a>\u003C/h3>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://i.postimg.cc/fbHW5XKr/00000-1765854059.png\" alt=\"Output Generation\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The image resembles Van Gogh&#39;s painting style and also it was able to capture the yellow tints from his sunflower painting. \u003C/p>\n\u003Ch2 id=\"animating-frames-with-animatediff\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Animating frames with AnimateDiff\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#animating-frames-with-animatediff\">#\u003C/a>\u003C/h2>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">This \u003Ca href=\"https://animatediff.github.io/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">paper\u003C/a> introduced very first kind of text-to-video prompting using Stable Diffusion. They prosed a framework that leveraged these \u003Cstrong>motion models\u003C/strong> to animate frames from existing diffusion model output thus generating video. At the time of writing this blog, there is a new and more enhanced text-to-video models called \u003Ca href=\"https://stability.ai/news/stable-video-diffusion-open-ai-video-model\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Stable Video Diffusion\u003C/a> which is from the same developers as Stable Diffusion. You can check them out as well. But for the purpose of this blogpost, I&#39;ll be covering on how to generate video frames using AnimateDiff.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Here is a sample of AnimateDiff.\u003C/p>\n\u003Cvideo width=\"640\" height=\"640\" controls>\n  \u003Csource src=\"https://animatediff.github.io/animations/teaser/ani_03.mp4\" type=\"video/mp4\">\n\u003C/video>\n\n\u003Ch3 id=\"setup---animatediff-extension\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Setup - AnimateDiff Extension\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#setup---animatediff-extension\">#\u003C/a>\u003C/h3>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Another factor contributing to the popularity of Web UI project is their extension support. Within the Open Source community, a plethora of plugins and extension support exists that adds advanced capabilities. One such extension that integrates AnimeDiff into Web UI is \u003Ca href=\"https://github.com/continue-revolution/sd-webui-animatediff\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">sd-webui-animatediff\u003C/a>. To incorporate this extension into your local Web UI setup, follow this \u003Ca href=\"https://github.com/continue-revolution/sd-webui-animatediff?tab=readme-ov-file#how-to-use\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">guide\u003C/a>. Download the required motion modules from the provided links, and you&#39;ll be ready to start using it.\u003C/p>\n\u003Ch3 id=\"usage---generating-video\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Usage - Generating Video\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#usage---generating-video\">#\u003C/a>\u003C/h3>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Navigate to the AnimateDiff configuration window, where you have the option to choose the Frames Per Second (FPS), total number of frames, and looping preferences for the animation frames. Additionally, you can specify the format for saving the video.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://i.ibb.co/LZPKccb/Screenshot-2024-02-12-135242.png\" alt=\"AnimateDiff Configuration\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Once you have chosen the necessary configuration and motion models, proceed to generate the video using your initial text prompt. It&#39;s important to note that the output generation time will be longer compared to image generation. This is expected due to the higher number of frames (images) involved in the generation process and the need to maintain contextual coherence across these frames.\u003C/p>\n\u003Ch3 id=\"output-demo-2\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Output Demo 2\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#output-demo-2\">#\u003C/a>\u003C/h3>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://i.ibb.co/4PKz3rz/00000-3467508380.gif\" alt=\"Sunflower Animation\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">This is how you can animate still images using AnimateDiff. The quality of generation is not superior but, it can be improved with lots of fine-tuning with some tweaks.\u003C/p>\n\u003Ch1 id=\"conclusion\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Conclusion\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#conclusion\">#\u003C/a>\u003C/h1>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Finally, I would like to conclude by stating that we are on the verge of witnessing a tremendous shift in the digital content we consume online. From advertisements to cinema experiences in movie theaters, everything will incorporate some form of these Generative AI applications featuring unreal elements in their content. It is essential for us get acquainted with these technologies as they are will create new opportunities across various sectors.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">While this technology streamlines certain aspects of content creation in digital media, it also opens the door for individuals with malicious intent to utilize it negatively. The prevalence of DeepFakes is on the rise, fraudsters can easily clone voices for deceptive purposes, and verifying the authenticity of simple news content can become even more challenging.\u003C/p>\n\u003Ch1 id=\"software/tools\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Software/Tools\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#software/tools\">#\u003C/a>\u003C/h1>\n\u003Ch2 id=\"diffusion-gui\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Diffusion GUI\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#diffusion-gui\">#\u003C/a>\u003C/h2>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">SD Web UI\u003C/a>: Used in this tutorial\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/comfyanonymous/ComfyUI\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">ComfyUI\u003C/a>: Advanced diffusion GUI tool which takes graphical node based approach to design simple to complex workflows.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"web-ui-extensions\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Web UI Extensions\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#web-ui-extensions\">#\u003C/a>\u003C/h2>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/continue-revolution/sd-webui-animatediff\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">AnimateDiff\u003C/a>: Enable motion model based animation support in webui.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/Mikubill/sd-webui-controlnet\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">ControlNet\u003C/a>: Let&#39;s you to copy human pose, expression, colors, contents, etc of some reference image to your target image.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/CiaraStrawberry/TemporalKit\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">TemporalKit\u003C/a>: Temporal Kit is a helper extension for using EbSynth, an app that lets you style videos frame by frame.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"additional-links\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">Additional Links\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#additional-links\">#\u003C/a>\u003C/h2>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://kdenlive.org/en/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Kdenlive\u003C/a>: Advanced video editing software. Completely open source and free to use.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://ncs.io/music\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">NCS\u003C/a>: No Copyrightright Sound for uploading on YouTube/Twitch platforms.\u003C/li>\n\u003C/ul>\n"}},"uses":{"params":["slug"]}}];

					Promise.all([
						import("../../_app/immutable/entry/start.ff4a89b7.js"),
						import("../../_app/immutable/entry/app.b29e0e1c.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 4],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
