{"type":"data","nodes":[null,{"type":"data","data":[{"post":1},{"slug":2,"date":3,"date_formatted":4,"title":5,"description":6,"metadata":7,"file":55,"content":56},"small-language-models-(slms)-is-all-you-need","2024-01-09T00:00:00+05:30","Jan 9, 2024","Small Language Models (SLMs) is all you need","Harnessing the full potential of Language Models! Say Hello to Small Language Models!",{"title":5,"date":3,"draft":8,"description":6,"author":9,"showToc":10,"cover":11,"cover_alt":12,"tags":13,"readingTime":18,"toc":19},"false","Me",true,"https://i.ibb.co/n3hwx4M/smol-llama-banner.png","Credit: \u003Ca class=\"underline underline-offset-4\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\">BEEspoke Data/smol-llama\u003C/a> ",[14,15,16,17],"AI/ML","LLM.js","LLM","NLP","5",[20,25,28,32,36,40,44,48,51],{"content":21,"slug":22,"lvl":23,"i":24,"seen":24},"Reasons Why Large Models Might be Overkill","reasons-why-large-models-might-be-overkill",1,0,{"content":26,"slug":27,"lvl":23,"i":23,"seen":24},"Streamlining Models with Quantization Techniques","streamlining-models-with-quantization-techniques",{"content":29,"slug":30,"lvl":31,"i":31,"seen":24},"LLAMA.CPP","llama.cpp",2,{"content":33,"slug":34,"lvl":31,"i":35,"seen":24},"What are my options for small language models?","what-are-my-options-for-small-language-models?",3,{"content":37,"slug":38,"lvl":35,"i":39,"seen":24},"TinyLlama (1.1B)","tinyllama-(1.1b)",4,{"content":41,"slug":42,"lvl":35,"i":43,"seen":24},"TinyMistral (248M)","tinymistral-(248m)",5,{"content":45,"slug":46,"lvl":35,"i":47,"seen":24},"Smol Llama (101M)","smol-llama-(101m)",6,{"content":15,"slug":49,"lvl":23,"i":50,"seen":24},"llm.js",7,{"content":52,"slug":53,"lvl":31,"i":54,"seen":24},"LLM.js - Playground","llm.js---playground",8,"2024\\small-language-models-is-all-you-need.md","\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Large Language Models (LLMs) dominated the market in 2023, and my prediction for this year is an increasing prevalence of even larger models with multi-modal capacities. These advanced models can seamlessly process audio, comprehend text, and generate data in various formats such as text, audio, or video. While large models excel in supporting a wide range of tasks and capabilities, they come at a cost – both in terms of CPU/GPU usage and your budget. Additionally, the expenses related to server capabilities, including hosting, power, and maintenance, are expected to rise in the coming years.\u003C/p>\n\u003Ch1 id=\"reasons-why-large-models-might-be-overkill\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Reasons Why Large Models Might be Overkill\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#reasons-why-large-models-might-be-overkill\">#\u003C/a>\u003C/h1>\n\u003Col class=\" space-y-1 list-decimal list-inside\" >\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Tailored Solutions for Specific Needs\u003C/strong>: While having the ability to reason extensively is beneficial, for most users developing applications, the focus is often on specific business or task-oriented use cases such as summarization, creative content writing, or translation. Not every scenario requires the complexity of a large network.\u003C/p>\n\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Fine-Tuning for Business:\u003C/strong> Traditional wisdom suggests that over 80% of the time, business use-cases requires fine-tuned models. Utilizing concepts like RAG to provide in-context information improves the quality of outputs, but a fine-tuned version tailored to the specific problem at hand consistently outperforms generic large models.\u003C/p>\n\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Cost Considerations\u003C/strong>: Operating on the cloud incurs expenses. Opting for smaller models and running them on-premises or at edge locations can significantly cut costs, making efficient use of resources without compromising performance and security.\u003C/p>\n\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Ease of Maintenance\u003C/strong>: Managing AI is akin to tinkering with an unknown force, leading to positive outcomes but also numerous failed experiments. Despite the mathematical reasoning behind AI, LLMs involve passing inputs through deep layers of neural networks, posing challenges in model explainability. As we move forward, breakthroughs in understanding these intricate black boxes are crucial to prevent tech saturation.\u003C/p>\n\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Enhanced Portability\u003C/strong>: Building smaller models with minimal computational demands enables easy portability to diverse hardware devices, including phones, smartwatches, tablets, Raspberry Pi, and more. Empowering IoT devices with intelligent models tailored to specific use cases becomes seamless and accessible.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch1 id=\"streamlining-models-with-quantization-techniques\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Streamlining Models with Quantization Techniques\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#streamlining-models-with-quantization-techniques\">#\u003C/a>\u003C/h1>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The concept of quantization has been in play since the introduction of the ONNX format. This technique optimizes and reduces model weights, effectively scaling down the overall model size. \u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The advent of Large Language Models (LLMs) brought forth a variety of model formats and optimization techniques aimed at efficiently reducing model size. The goal is to maintain model perplexity—assessing how well a probability model predicts a sample—while significantly reducing overall dimensions.\u003C/p>\n\u003Col class=\" space-y-1 list-decimal list-inside\" >\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Activation Aware Quantization (AWQ)\u003C/strong>: AWQ safeguards critical weights by optimizing per-channel scaling based on activation rather than weights. Demonstrating superior performance on language modeling and QA benchmarks, AWQ excels in quantization, making it an ideal solution for compressing models to 3/4 bits, ensuring efficient deployment in various applications.\u003C/p>\n\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Floating Point/Arithmetic Quantization (FP)\u003C/strong>: Typically, models are built using high-precision FP32 datatypes. However, the aim of this quantization is to shift the model from FP32 to lower datatypes (FP16, int8, int4), representing a normalized version of the original values. Although this trade-off leads to reduced precision, affecting the quality of the model output, it facilitates a substantial reduction in size.\u003C/p>\n\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Post Training Quantization for GPT (GPTQ)\u003C/strong>: This is a one-shot weight quantization method designed for Generative Pre-trained Transformer (GPT) models. GPTQ can quantize models with 175 billion parameters to 3 or 4 bits per weight, resulting in negligible accuracy degradation. In comparison to previous methods, it offers significant compression gains. This technique enables the execution of large models on a single GPU for generative inference, leading to notable inference speedups over FP16. Read more: \u003Ca href=\"https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">here\u003C/a>.\u003C/p>\n\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>GGML/GGUF\u003C/strong>: GGML is a machine learning library written in C, created by \u003Ca href=\"https://github.com/ggerganov\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Georgi Gerganov\u003C/a>. The library provides foundational elements for creating tensor based neural networks, but also a \u003Cstrong>unique binary format\u003C/strong> to distribute LLMs. \u003Cem>GGML\u003C/em> served as the initial format of this binary format and it recently translated to \u003Cem>GGUF\u003C/em> as to introduces broader range of parameters, custom tokenizer features, etc making it future proof. The library offers various levels of quantization, where it combines tensors and reduces down the scale (floating point precision) of some group thus yielding to reduction in model size. Read more: \u003Ca href=\"https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">here\u003C/a>\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cem>@oobabooga\u003C/em> (Founder of text-generation-webui) has published a detailed \u003Ca href=\"https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">report\u003C/a> on the comparison of different quantization techniques.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://i.imgur.com/Q0N9Rja.png\" alt=\"Model Perplexity\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">As you can see there is a significant reduction in memory as the model loses its perplexity.\u003C/p>\n\u003Ch2 id=\"llama.cpp\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">LLAMA.CPP\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#llama.cpp\">#\u003C/a>\u003C/h2>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Originally, this project emerged to execute LLAMA series models on CPU devices by compiling them into machine code specific to those devices. Over time, the project has expanded significantly to accommodate various model architectures, enabling execution on diverse hardware devices such as MAC, ARM, CUDA, and more. It is constructed with a focus on optimizing performance. Notably, the project has introduced a new format, GGML/GGUF, for storing and sharing the original model weights in a compressed format. Visualize it as a unified binary file encapsulating essential elements like tokenizers, models, configurations, vocabularies, etc., required for running your model—all neatly packed into a single format. GGUF succeeds GGML and extends its support to non-LLAMA models, incorporating token-extension capabilities. I highly recommend exploring this development further.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Project: \u003Ca href=\"https://github.com/ggerganov/llama.cpp\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">link\u003C/a>\u003C/p>\n\u003Ch2 id=\"what-are-my-options-for-small-language-models?\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">What are my options for small language models?\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#what-are-my-options-for-small-language-models?\">#\u003C/a>\u003C/h2>\n\u003Ch3 id=\"tinyllama-(1.1b)\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">TinyLlama (1.1B)\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#tinyllama-(1.1b)\">#\u003C/a>\u003C/h3>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">Same architecture and tokenizer as Llama 2 but parameters scaled down.\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">Trained on 3 Trillion Tokens.\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">Supports sequence lengths of 2048.\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Ca href=\"https://github.com/jzhang38/TinyLlama\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Github\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"tinymistral-(248m)\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">TinyMistral (248M)\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#tinymistral-(248m)\">#\u003C/a>\u003C/h3>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">Based on Mistral architecture.\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">The base model may not be suitable for conversation, but there are variants like the &quot;Instruct&quot; version designed to support chat applications.\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">Supports sequence lengths of up to 32k.\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Ca href=\"https://huggingface.co/Locutusque/TinyMistral-248M\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Github\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"smol-llama-(101m)\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Smol Llama (101M)\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#smol-llama-(101m)\">#\u003C/a>\u003C/h3>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">A smaller version of the Llama model trained from scratch on different datasets.\u003C/li>\n\u003Cli class=\" text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Ca href=\"https://huggingface.co/Felladrin/Smol-Llama-101M-Chat-v1\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Github\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Ch1 id=\"llm.js\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">LLM.js\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#llm.js\">#\u003C/a>\u003C/h1>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Ca href=\"https://rahuldshetty.github.io/llm.js/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">LLM.js\u003C/a> is my side hobby project, designed to facilitate the execution of Language Models in various inference formats on the web. Inspired by transformers.js, which aims to bring the transformer ecosystem into browsers, LLM.js is more specific, focusing on text-generation use cases with different language model formats and architectures.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The project owes its existence to the collaborative efforts of the community, particularly on projects like LLAMA.CPP, llama.c, and Emscripten. Emscripten enables the compilation of projects into the WebAssembly (WASM) format, allowing them to be executed in a web browser&#39;s WebAssembly engine.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://storage.googleapis.com/blog-images-backup/0*xNUJSNFWssBQcekQ.png\" alt=\"WASM Flow\">\u003C/p>\n\u003Ch2 id=\"llm.js---playground\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">LLM.js - Playground\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#llm.js---playground\">#\u003C/a>\u003C/h2>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Ca href=\"https://rahuldshetty.github.io/ggml.js-examples/playground.html\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Playground\u003C/a> simplifies the process of testing various Language Models (LM) directly in your web browser. No additional dependencies are required, apart from a standard web browser (which should support WASM, a feature most modern browsers include).\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The playground allows you to effortlessly fetch supported models from Hugging Face and execute them in your browser. You can customize the initial prompt and engage in text generation. While currently, direct modification of model parameters is not supported, this functionality is actively under development for the playground UI.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Playground Demo: \u003Ca href=\"https://rahuldshetty.github.io/ggml.js-examples/playground.html\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">link\u003C/a>\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Source Code: \u003Ca href=\"https://github.com/rahuldshetty/ggml.js-examples/blob/master/playground.html\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Playground\u003C/a>\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://raw.githubusercontent.com/rahuldshetty/ggml.js-examples/master/llm-js.gif\" alt=\"LLM.js Playground\">\u003C/p>\n"],"uses":{"params":["slug"]}}]}
