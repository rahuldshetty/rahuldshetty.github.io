<script>
	// It's best to inline this in `head` to avoid FOUC (flash of unstyled content) when changing pages or themes
	if (
	  localStorage.getItem('color-theme') === 'dark' ||
	  (!('color-theme' in localStorage) &&
		window.matchMedia('(prefers-color-scheme: dark)').matches)
	) {
	  document.documentElement.classList.add('dark');
	} else {
	  document.documentElement.classList.remove('dark');
	}
  </script>

<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../../favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../../_app/immutable/assets/0.ab550426.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/fa.95b16411.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/4.e956f1fb.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.92df42a1.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/scheduler.97c8b5c4.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/singletons.9f746391.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.f08e7f04.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/paths.3df2068f.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.705885d4.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/preload-helper.a4192956.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.9ebb7127.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.4f6899d2.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/each.e59479a4.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/stores.395c5a03.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/fa.2329cfa5.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.e870d26e.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/4.75b35614.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/cover.2889fc35.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.7a3d2515.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index.73062db0.js"><title>Small Language Models (SLMs) is all you need</title><!-- HEAD_svelte-fgg0v1_START --><script async src="https://www.googletagmanager.com/gtag/js?id=G-WVR6SWQH0Z" data-svelte-h="svelte-1thcm6j"></script><!-- HEAD_svelte-fgg0v1_END --><!-- HEAD_svelte-1n6hdy6_START --><meta property="og:site_name" content="RDS's Blog"><meta property="og:title" content="Small Language Models (SLMs) is all you need"><meta property="og:type" content="article"><meta property="og:description" content="Harnessing the full potential of Language Models! Say Hello to Small Language Models!"><meta property="og:image" content="https://i.ibb.co/n3hwx4M/smol-llama-banner.png"><meta property="article:published_time" content="2024-01-09T00:00:00+05:30"><meta name="twitter:card" content="summary_large_image"><!-- HEAD_svelte-1n6hdy6_END -->
	</head>
	<body data-sveltekit-preload-data="hover" 
		class="dark:bg-[#121212] dark:border-gray-50 flex flex-col min-h-screen transition-colors duration-500"
	>
		<div style="display: contents">   <nav class="dark:bg-[#121212] transition-colors duration-0"><div class="mx-auto max-w-7xl px-2 sm:px-6 lg:px-8"><div class="relative flex h-16 items-center justify-between"><div class="absolute inset-y-0 left-0 flex items-center sm:hidden"> <button type="button" class="relative inline-flex items-center justify-center rounded-md p-2 text-gray-400 focus:outline-none" aria-controls="mobile-menu" aria-expanded="false" data-svelte-h="svelte-1pv4qi8"><span class="absolute -inset-0.5"></span> <span class="sr-only">Open Menu</span>  <svg class="block h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg>  <svg class="hidden h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true"><path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12"></path></svg></button></div> <div class="flex flex-1 items-center justify-center sm:items-stretch sm:justify-start"><div class="flex flex-shrink-0 items-center" data-svelte-h="svelte-frp9wu"><a href="/"><img class="h-8 w-auto" src="/_app/immutable/assets/logo.33e33a6c.png" alt="Website Logo"></a></div> <div class="hidden sm:ml-6 sm:block"> <div class="flex space-x-4"> <a href="/" target="_self" class="text-gray-400 dark:text-[#b3b3b3] hover:bg-gray-700 hover:text-white rounded-md px-3 py-2 text-sm font-medium" aria-current="page"><div class="flex flex-row items-center"> Portfolio</div> </a> <a href="/blog/" target="_self" class="text-gray-700 dark:text-[#b3b3b3] underline underline-offset-4 decoration-2 dark:text-[#b3b3b3] hover:bg-gray-700 hover:text-white rounded-md px-3 py-2 text-sm font-medium" aria-current="page"><div class="flex flex-row items-center"> Blog</div> </a> <a href="/gallery/" target="_self" class="text-gray-400 dark:text-[#b3b3b3] hover:bg-gray-700 hover:text-white rounded-md px-3 py-2 text-sm font-medium" aria-current="page"><div class="flex flex-row items-center"> Gallery</div> </a> <a href="/_app/immutable/assets/resume.51e97c55.pdf" target="_blank" class="text-gray-400 dark:text-[#b3b3b3] hover:bg-gray-700 hover:text-white rounded-md px-3 py-2 text-sm font-medium" aria-current="page"><div class="flex flex-row items-center"><svg class="svelte-fa mr-2 svelte-1cj2gr0" style="height:1em;font-size:.75em;vertical-align:-.125em;transform-origin:center;overflow:visible" viewBox="0 0 512 512" aria-hidden="true" role="img" xmlns="http://www.w3.org/2000/svg"><g transform="translate(256 256)" transform-origin="128 0"><g transform="translate(0,0) scale(1,1)"><path d="M352 0c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9L370.7 96 201.4 265.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L416 141.3l41.4 41.4c9.2 9.2 22.9 11.9 34.9 6.9s19.8-16.6 19.8-29.6V32c0-17.7-14.3-32-32-32H352zM80 32C35.8 32 0 67.8 0 112V432c0 44.2 35.8 80 80 80H400c44.2 0 80-35.8 80-80V320c0-17.7-14.3-32-32-32s-32 14.3-32 32V432c0 8.8-7.2 16-16 16H80c-8.8 0-16-7.2-16-16V112c0-8.8 7.2-16 16-16H192c17.7 0 32-14.3 32-32s-14.3-32-32-32H80z" fill="currentColor" transform="translate(-256 -256)"></path></g></g></svg> Resume</div> </a>  <button id="theme-toggle" type="button" class="text-gray-500 dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:ring-gray-200 dark:focus:ring-gray-700 rounded-full text-sm p-2.5"><svg id="theme-toggle-dark-icon" class="svelte-fa hidden svelte-1cj2gr0" style="height:1em;vertical-align:-.125em;transform-origin:center;overflow:visible" viewBox="0 0 384 512" aria-hidden="true" role="img" xmlns="http://www.w3.org/2000/svg"><g transform="translate(192 256)" transform-origin="96 0"><g transform="translate(0,0) scale(1,1)"><path d="M223.5 32C100 32 0 132.3 0 256S100 480 223.5 480c60.6 0 115.5-24.2 155.8-63.4c5-4.9 6.3-12.5 3.1-18.7s-10.1-9.7-17-8.5c-9.8 1.7-19.8 2.6-30.1 2.6c-96.9 0-175.5-78.8-175.5-176c0-65.8 36-123.1 89.3-153.3c6.1-3.5 9.2-10.5 7.7-17.3s-7.3-11.9-14.3-12.5c-6.3-.5-12.6-.8-19-.8z" fill="gray" transform="translate(-192 -256)"></path></g></g></svg> <svg id="theme-toggle-light-icon" class="svelte-fa hidden svelte-1cj2gr0" style="height:1em;vertical-align:-.125em;transform-origin:center;overflow:visible" viewBox="0 0 512 512" aria-hidden="true" role="img" xmlns="http://www.w3.org/2000/svg"><g transform="translate(256 256)" transform-origin="128 0"><g transform="translate(0,0) scale(1,1)"><path d="M361.5 1.2c5 2.1 8.6 6.6 9.6 11.9L391 121l107.9 19.8c5.3 1 9.8 4.6 11.9 9.6s1.5 10.7-1.6 15.2L446.9 256l62.3 90.3c3.1 4.5 3.7 10.2 1.6 15.2s-6.6 8.6-11.9 9.6L391 391 371.1 498.9c-1 5.3-4.6 9.8-9.6 11.9s-10.7 1.5-15.2-1.6L256 446.9l-90.3 62.3c-4.5 3.1-10.2 3.7-15.2 1.6s-8.6-6.6-9.6-11.9L121 391 13.1 371.1c-5.3-1-9.8-4.6-11.9-9.6s-1.5-10.7 1.6-15.2L65.1 256 2.8 165.7c-3.1-4.5-3.7-10.2-1.6-15.2s6.6-8.6 11.9-9.6L121 121 140.9 13.1c1-5.3 4.6-9.8 9.6-11.9s10.7-1.5 15.2 1.6L256 65.1 346.3 2.8c4.5-3.1 10.2-3.7 15.2-1.6zM160 256a96 96 0 1 1 192 0 96 96 0 1 1 -192 0zm224 0a128 128 0 1 0 -256 0 128 128 0 1 0 256 0z" fill="yellow" transform="translate(-256 -256)"></path></g></g></svg></button></div></div></div></div></div>  <div class="sm:hidden hidden" id="mobile-menu"><div class="space-y-1 px-2 pb-3 pt-2"><a href="/" target="_self" class="text-gray-300 hover:bg-gray-700 hover:text-white block rounded-md px-3 py-2 text-base font-medium text-gray-400"><div class="flex flex-row items-center"> Portfolio</div> </a><a href="/blog/" target="_self" class="text-gray-300 hover:bg-gray-700 hover:text-white block rounded-md px-3 py-2 text-base font-medium text-gray-700"><div class="flex flex-row items-center"> Blog</div> </a><a href="/gallery/" target="_self" class="text-gray-300 hover:bg-gray-700 hover:text-white block rounded-md px-3 py-2 text-base font-medium text-gray-400"><div class="flex flex-row items-center"> Gallery</div> </a><a href="/_app/immutable/assets/resume.51e97c55.pdf" target="_blank" class="text-gray-300 hover:bg-gray-700 hover:text-white block rounded-md px-3 py-2 text-base font-medium text-gray-400"><div class="flex flex-row items-center"><svg class="svelte-fa mr-2 svelte-1cj2gr0" style="height:1em;font-size:.75em;vertical-align:-.125em;transform-origin:center;overflow:visible" viewBox="0 0 512 512" aria-hidden="true" role="img" xmlns="http://www.w3.org/2000/svg"><g transform="translate(256 256)" transform-origin="128 0"><g transform="translate(0,0) scale(1,1)"><path d="M352 0c-12.9 0-24.6 7.8-29.6 19.8s-2.2 25.7 6.9 34.9L370.7 96 201.4 265.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L416 141.3l41.4 41.4c9.2 9.2 22.9 11.9 34.9 6.9s19.8-16.6 19.8-29.6V32c0-17.7-14.3-32-32-32H352zM80 32C35.8 32 0 67.8 0 112V432c0 44.2 35.8 80 80 80H400c44.2 0 80-35.8 80-80V320c0-17.7-14.3-32-32-32s-32 14.3-32 32V432c0 8.8-7.2 16-16 16H80c-8.8 0-16-7.2-16-16V112c0-8.8 7.2-16 16-16H192c17.7 0 32-14.3 32-32s-14.3-32-32-32H80z" fill="currentColor" transform="translate(-256 -256)"></path></g></g></svg> Resume</div> </a></div></div></nav> <div class="container mx-auto m-4 overscroll-none justify-center items-center flex relative"> <div class="flex flex-col max-w-2xl max-[600px]:p-10"><h1 class="text-4xl font-bold text-gray-900 place-self-start dark:text-neutral-100">Small Language Models (SLMs) is all you need</h1> <p class="text-xl my-1 text-neutral-500">Harnessing the full potential of Language Models! Say Hello to Small Language Models!</p> <div class="flex flex-row"><span class="inline-flex items-center rounded-md bg-blue-50 font-medium text-blue-600 dark:bg-blue-50 dark:text-neutral-800 px-2 py-1 mr-1 my-1 text-xs ring-1 ring-inset ring-gray-500/10">AI/ML</span><span class="inline-flex items-center rounded-md bg-blue-50 font-medium text-blue-600 dark:bg-blue-50 dark:text-neutral-800 px-2 py-1 mr-1 my-1 text-xs ring-1 ring-inset ring-gray-500/10">LLM.js</span><span class="inline-flex items-center rounded-md bg-blue-50 font-medium text-blue-600 dark:bg-blue-50 dark:text-neutral-800 px-2 py-1 mr-1 my-1 text-xs ring-1 ring-inset ring-gray-500/10">LLM</span><span class="inline-flex items-center rounded-md bg-blue-50 font-medium text-blue-600 dark:bg-blue-50 dark:text-neutral-800 px-2 py-1 mr-1 my-1 text-xs ring-1 ring-inset ring-gray-500/10">NLP</span></div> <div class="flex flex-row"><small class="text-neutral-500 dark:text-neutral-600 my-1">Jan 9, 2024</small> <span class="text-neutral-700 dark:text-neutral-700 mx-1" data-svelte-h="svelte-10lih0p">·</span> <small class="text-neutral-500 dark:text-neutral-600 my-1">5 mins</small></div> <div class="flex flex-col place-items-center"><img class="object-cover rounded-lg py-1" src="https://i.ibb.co/n3hwx4M/smol-llama-banner.png" alt="Credit: <a class=&quot;underline underline-offset-4&quot; href=&quot;https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA&quot;>BEEspoke Data/smol-llama</a> "> <small class="text-neutral-500 dark:text-neutral-600"><!-- HTML_TAG_START -->Credit: <a class="underline underline-offset-4" href="https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA">BEEspoke Data/smol-llama</a> <!-- HTML_TAG_END --></small></div> <div class=""><!-- HTML_TAG_START --><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Large Language Models (LLMs) dominated the market in 2023, and my prediction for this year is an increasing prevalence of even larger models with multi-modal capacities. These advanced models can seamlessly process audio, comprehend text, and generate data in various formats such as text, audio, or video. While large models excel in supporting a wide range of tasks and capabilities, they come at a cost – both in terms of CPU/GPU usage and your budget. Additionally, the expenses related to server capabilities, including hosting, power, and maintenance, are expected to rise in the coming years.</p>
<h1 id="reasons-why-large-models-might-be-overkill" class="group text-3xl font-bold mt-4 dark:text-neutral-200">Reasons Why Large Models Might be Overkill<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#reasons-why-large-models-might-be-overkill">#</a></h1>
<ol class=" space-y-1 list-decimal list-inside" >
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Tailored Solutions for Specific Needs</strong>: While having the ability to reason extensively is beneficial, for most users developing applications, the focus is often on specific business or task-oriented use cases such as summarization, creative content writing, or translation. Not every scenario requires the complexity of a large network.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Fine-Tuning for Business:</strong> Traditional wisdom suggests that over 80% of the time, business use-cases requires fine-tuned models. Utilizing concepts like RAG to provide in-context information improves the quality of outputs, but a fine-tuned version tailored to the specific problem at hand consistently outperforms generic large models.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Cost Considerations</strong>: Operating on the cloud incurs expenses. Opting for smaller models and running them on-premises or at edge locations can significantly cut costs, making efficient use of resources without compromising performance and security.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Ease of Maintenance</strong>: Managing AI is akin to tinkering with an unknown force, leading to positive outcomes but also numerous failed experiments. Despite the mathematical reasoning behind AI, LLMs involve passing inputs through deep layers of neural networks, posing challenges in model explainability. As we move forward, breakthroughs in understanding these intricate black boxes are crucial to prevent tech saturation.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Enhanced Portability</strong>: Building smaller models with minimal computational demands enables easy portability to diverse hardware devices, including phones, smartwatches, tablets, Raspberry Pi, and more. Empowering IoT devices with intelligent models tailored to specific use cases becomes seamless and accessible.</p>
</li>
</ol>
<h1 id="streamlining-models-with-quantization-techniques" class="group text-3xl font-bold mt-4 dark:text-neutral-200">Streamlining Models with Quantization Techniques<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#streamlining-models-with-quantization-techniques">#</a></h1>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">The concept of quantization has been in play since the introduction of the ONNX format. This technique optimizes and reduces model weights, effectively scaling down the overall model size. </p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">The advent of Large Language Models (LLMs) brought forth a variety of model formats and optimization techniques aimed at efficiently reducing model size. The goal is to maintain model perplexity—assessing how well a probability model predicts a sample—while significantly reducing overall dimensions.</p>
<ol class=" space-y-1 list-decimal list-inside" >
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Activation Aware Quantization (AWQ)</strong>: AWQ safeguards critical weights by optimizing per-channel scaling based on activation rather than weights. Demonstrating superior performance on language modeling and QA benchmarks, AWQ excels in quantization, making it an ideal solution for compressing models to 3/4 bits, ensuring efficient deployment in various applications.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Floating Point/Arithmetic Quantization (FP)</strong>: Typically, models are built using high-precision FP32 datatypes. However, the aim of this quantization is to shift the model from FP32 to lower datatypes (FP16, int8, int4), representing a normalized version of the original values. Although this trade-off leads to reduced precision, affecting the quality of the model output, it facilitates a substantial reduction in size.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>Post Training Quantization for GPT (GPTQ)</strong>: This is a one-shot weight quantization method designed for Generative Pre-trained Transformer (GPT) models. GPTQ can quantize models with 175 billion parameters to 3 or 4 bits per weight, resulting in negligible accuracy degradation. In comparison to previous methods, it offers significant compression gains. This technique enables the execution of large models on a single GPU for generative inference, leading to notable inference speedups over FP16. Read more: <a href="https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">here</a>.</p>
</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><strong>GGML/GGUF</strong>: GGML is a machine learning library written in C, created by <a href="https://github.com/ggerganov" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">Georgi Gerganov</a>. The library provides foundational elements for creating tensor based neural networks, but also a <strong>unique binary format</strong> to distribute LLMs. <em>GGML</em> served as the initial format of this binary format and it recently translated to <em>GGUF</em> as to introduces broader range of parameters, custom tokenizer features, etc making it future proof. The library offers various levels of quantization, where it combines tensors and reduces down the scale (floating point precision) of some group thus yielding to reduction in model size. Read more: <a href="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">here</a></p>
</li>
</ol>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><em>@oobabooga</em> (Founder of text-generation-webui) has published a detailed <a href="https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">report</a> on the comparison of different quantization techniques.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><img src="https://i.imgur.com/Q0N9Rja.png" alt="Model Perplexity"></p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">As you can see there is a significant reduction in memory as the model loses its perplexity.</p>
<h2 id="llama.cpp" class="group text-2xl font-bold  mt-4 dark:text-neutral-200">LLAMA.CPP<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#llama.cpp">#</a></h2>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Originally, this project emerged to execute LLAMA series models on CPU devices by compiling them into machine code specific to those devices. Over time, the project has expanded significantly to accommodate various model architectures, enabling execution on diverse hardware devices such as MAC, ARM, CUDA, and more. It is constructed with a focus on optimizing performance. Notably, the project has introduced a new format, GGML/GGUF, for storing and sharing the original model weights in a compressed format. Visualize it as a unified binary file encapsulating essential elements like tokenizers, models, configurations, vocabularies, etc., required for running your model—all neatly packed into a single format. GGUF succeeds GGML and extends its support to non-LLAMA models, incorporating token-extension capabilities. I highly recommend exploring this development further.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Project: <a href="https://github.com/ggerganov/llama.cpp" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">link</a></p>
<h2 id="what-are-my-options-for-small-language-models?" class="group text-2xl font-bold  mt-4 dark:text-neutral-200">What are my options for small language models?<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#what-are-my-options-for-small-language-models?">#</a></h2>
<h3 id="tinyllama-(1.1b)" class="group text-xl font-bold  mt-4 dark:text-neutral-200">TinyLlama (1.1B)<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#tinyllama-(1.1b)">#</a></h3>
<ul class=" space-y-1 list-disc list-inside" >
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm ">Same architecture and tokenizer as Llama 2 but parameters scaled down.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm ">Trained on 3 Trillion Tokens.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm ">Supports sequence lengths of 2048.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://github.com/jzhang38/TinyLlama" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">Github</a></li>
</ul>
<h3 id="tinymistral-(248m)" class="group text-xl font-bold  mt-4 dark:text-neutral-200">TinyMistral (248M)<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#tinymistral-(248m)">#</a></h3>
<ul class=" space-y-1 list-disc list-inside" >
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm ">Based on Mistral architecture.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm ">The base model may not be suitable for conversation, but there are variants like the &quot;Instruct&quot; version designed to support chat applications.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm ">Supports sequence lengths of up to 32k.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://huggingface.co/Locutusque/TinyMistral-248M" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">Github</a></li>
</ul>
<h3 id="smol-llama-(101m)" class="group text-xl font-bold  mt-4 dark:text-neutral-200">Smol Llama (101M)<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#smol-llama-(101m)">#</a></h3>
<ul class=" space-y-1 list-disc list-inside" >
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm ">A smaller version of the Llama model trained from scratch on different datasets.</li>
<li class="text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm "><a href="https://huggingface.co/Felladrin/Smol-Llama-101M-Chat-v1" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">Github</a></li>
</ul>
<h1 id="llm.js" class="group text-3xl font-bold mt-4 dark:text-neutral-200">LLM.js<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#llm.js">#</a></h1>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><a href="https://rahuldshetty.github.io/llm.js/" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">LLM.js</a> is my side hobby project, designed to facilitate the execution of Language Models in various inference formats on the web. Inspired by transformers.js, which aims to bring the transformer ecosystem into browsers, LLM.js is more specific, focusing on text-generation use cases with different language model formats and architectures.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">The project owes its existence to the collaborative efforts of the community, particularly on projects like LLAMA.CPP, llama.c, and Emscripten. Emscripten enables the compilation of projects into the WebAssembly (WASM) format, allowing them to be executed in a web browser&#39;s WebAssembly engine.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><img src="https://storage.googleapis.com/blog-images-backup/0*xNUJSNFWssBQcekQ.png" alt="WASM Flow"></p>
<h2 id="llm.js---playground" class="group text-2xl font-bold  mt-4 dark:text-neutral-200">LLM.js - Playground<a class="mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700" href="#llm.js---playground">#</a></h2>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><a href="https://rahuldshetty.github.io/ggml.js-examples/playground.html" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">Playground</a> simplifies the process of testing various Language Models (LM) directly in your web browser. No additional dependencies are required, apart from a standard web browser (which should support WASM, a feature most modern browsers include).</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">The playground allows you to effortlessly fetch supported models from Hugging Face and execute them in your browser. You can customize the initial prompt and engage in text generation. While currently, direct modification of model parameters is not supported, this functionality is actively under development for the playground UI.</p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Playground Demo: <a href="https://rahuldshetty.github.io/ggml.js-examples/playground.html" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">link</a></p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose">Source Code: <a href="https://github.com/rahuldshetty/ggml.js-examples/blob/master/playground.html" class="underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200">Playground</a></p>
<p class="mt-4 text-neutral-700 dark:text-neutral-300 leading-loose"><img src="https://raw.githubusercontent.com/rahuldshetty/ggml.js-examples/master/llm-js.gif" alt="LLM.js Playground"></p>
<!-- HTML_TAG_END --></div></div>    <div class="fixed right-3/4 w-64 max-[600px]:hidden" style="top: 15%;"><a href="#reasons-why-large-models-might-be-overkill"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-2">Reasons Why Large Models Might be Overkill</h4> </a><a href="#streamlining-models-with-quantization-techniques"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-2">Streamlining Models with Quantization Techniques</h4> </a><a href="#llama.cpp"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-8">LLAMA.CPP</h4> </a><a href="#what-are-my-options-for-small-language-models?"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-8">What are my options for small language models?</h4> </a><a href="#tinyllama-(1.1b)"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-16">TinyLlama (1.1B)</h4> </a><a href="#tinymistral-(248m)"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-16">TinyMistral (248M)</h4> </a><a href="#smol-llama-(101m)"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-16">Smol Llama (101M)</h4> </a><a href="#llm.js"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-2">LLM.js</h4> </a><a href="#llm.js---playground"><h4 class="text-base underline text-ellipsis decoration-sky-500 underline-offset-4 text-gray-900 dark:text-sky-500 hover:text-sky-500 dark:hover:text-gray-200 transition duration-300 ease-in-out font-bold my-2 mx-8">LLM.js - Playground</h4> </a></div></div> <footer class="text-center lg:text-left mt-auto"><div class="p-4 text-xs text-center text-neutral-400 dark:text-neutral-200">© 2024 <a class="text-xs dark:text-neutral-200 text-neutral-400 underline underline-offset-4" href="/" data-svelte-h="svelte-1h1d2v0">rds</a></div></footer> 
			
			<script>
				{
					__sveltekit_1kojvjt = {
						base: new URL("../..", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,{"type":"data","data":{post:{slug:"small-language-models-(slms)-is-all-you-need",date:"2024-01-09T00:00:00+05:30",date_formatted:"Jan 9, 2024",title:"Small Language Models (SLMs) is all you need",description:"Harnessing the full potential of Language Models! Say Hello to Small Language Models!",metadata:{title:"Small Language Models (SLMs) is all you need",date:"2024-01-09T00:00:00+05:30",draft:"false",description:"Harnessing the full potential of Language Models! Say Hello to Small Language Models!",author:"Me",showToc:true,cover:"https://i.ibb.co/n3hwx4M/smol-llama-banner.png",cover_alt:"Credit: \u003Ca class=\"underline underline-offset-4\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\">BEEspoke Data/smol-llama\u003C/a> ",tags:["AI/ML","LLM.js","LLM","NLP"],readingTime:"5",toc:[{content:"Reasons Why Large Models Might be Overkill",slug:"reasons-why-large-models-might-be-overkill",lvl:1,i:0,seen:0},{content:"Streamlining Models with Quantization Techniques",slug:"streamlining-models-with-quantization-techniques",lvl:1,i:1,seen:0},{content:"LLAMA.CPP",slug:"llama.cpp",lvl:2,i:2,seen:0},{content:"What are my options for small language models?",slug:"what-are-my-options-for-small-language-models?",lvl:2,i:3,seen:0},{content:"TinyLlama (1.1B)",slug:"tinyllama-(1.1b)",lvl:3,i:4,seen:0},{content:"TinyMistral (248M)",slug:"tinymistral-(248m)",lvl:3,i:5,seen:0},{content:"Smol Llama (101M)",slug:"smol-llama-(101m)",lvl:3,i:6,seen:0},{content:"LLM.js",slug:"llm.js",lvl:1,i:7,seen:0},{content:"LLM.js - Playground",slug:"llm.js---playground",lvl:2,i:8,seen:0}]},file:"2024\\small-language-models-is-all-you-need.md",content:"\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Large Language Models (LLMs) dominated the market in 2023, and my prediction for this year is an increasing prevalence of even larger models with multi-modal capacities. These advanced models can seamlessly process audio, comprehend text, and generate data in various formats such as text, audio, or video. While large models excel in supporting a wide range of tasks and capabilities, they come at a cost – both in terms of CPU/GPU usage and your budget. Additionally, the expenses related to server capabilities, including hosting, power, and maintenance, are expected to rise in the coming years.\u003C/p>\n\u003Ch1 id=\"reasons-why-large-models-might-be-overkill\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Reasons Why Large Models Might be Overkill\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#reasons-why-large-models-might-be-overkill\">#\u003C/a>\u003C/h1>\n\u003Col class=\" space-y-1 list-decimal list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Tailored Solutions for Specific Needs\u003C/strong>: While having the ability to reason extensively is beneficial, for most users developing applications, the focus is often on specific business or task-oriented use cases such as summarization, creative content writing, or translation. Not every scenario requires the complexity of a large network.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Fine-Tuning for Business:\u003C/strong> Traditional wisdom suggests that over 80% of the time, business use-cases requires fine-tuned models. Utilizing concepts like RAG to provide in-context information improves the quality of outputs, but a fine-tuned version tailored to the specific problem at hand consistently outperforms generic large models.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Cost Considerations\u003C/strong>: Operating on the cloud incurs expenses. Opting for smaller models and running them on-premises or at edge locations can significantly cut costs, making efficient use of resources without compromising performance and security.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Ease of Maintenance\u003C/strong>: Managing AI is akin to tinkering with an unknown force, leading to positive outcomes but also numerous failed experiments. Despite the mathematical reasoning behind AI, LLMs involve passing inputs through deep layers of neural networks, posing challenges in model explainability. As we move forward, breakthroughs in understanding these intricate black boxes are crucial to prevent tech saturation.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Enhanced Portability\u003C/strong>: Building smaller models with minimal computational demands enables easy portability to diverse hardware devices, including phones, smartwatches, tablets, Raspberry Pi, and more. Empowering IoT devices with intelligent models tailored to specific use cases becomes seamless and accessible.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch1 id=\"streamlining-models-with-quantization-techniques\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">Streamlining Models with Quantization Techniques\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#streamlining-models-with-quantization-techniques\">#\u003C/a>\u003C/h1>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The concept of quantization has been in play since the introduction of the ONNX format. This technique optimizes and reduces model weights, effectively scaling down the overall model size. \u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The advent of Large Language Models (LLMs) brought forth a variety of model formats and optimization techniques aimed at efficiently reducing model size. The goal is to maintain model perplexity—assessing how well a probability model predicts a sample—while significantly reducing overall dimensions.\u003C/p>\n\u003Col class=\" space-y-1 list-decimal list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Activation Aware Quantization (AWQ)\u003C/strong>: AWQ safeguards critical weights by optimizing per-channel scaling based on activation rather than weights. Demonstrating superior performance on language modeling and QA benchmarks, AWQ excels in quantization, making it an ideal solution for compressing models to 3/4 bits, ensuring efficient deployment in various applications.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Floating Point/Arithmetic Quantization (FP)\u003C/strong>: Typically, models are built using high-precision FP32 datatypes. However, the aim of this quantization is to shift the model from FP32 to lower datatypes (FP16, int8, int4), representing a normalized version of the original values. Although this trade-off leads to reduced precision, affecting the quality of the model output, it facilitates a substantial reduction in size.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>Post Training Quantization for GPT (GPTQ)\u003C/strong>: This is a one-shot weight quantization method designed for Generative Pre-trained Transformer (GPT) models. GPTQ can quantize models with 175 billion parameters to 3 or 4 bits per weight, resulting in negligible accuracy degradation. In comparison to previous methods, it offers significant compression gains. This technique enables the execution of large models on a single GPU for generative inference, leading to notable inference speedups over FP16. Read more: \u003Ca href=\"https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">here\u003C/a>.\u003C/p>\n\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cstrong>GGML/GGUF\u003C/strong>: GGML is a machine learning library written in C, created by \u003Ca href=\"https://github.com/ggerganov\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Georgi Gerganov\u003C/a>. The library provides foundational elements for creating tensor based neural networks, but also a \u003Cstrong>unique binary format\u003C/strong> to distribute LLMs. \u003Cem>GGML\u003C/em> served as the initial format of this binary format and it recently translated to \u003Cem>GGUF\u003C/em> as to introduces broader range of parameters, custom tokenizer features, etc making it future proof. The library offers various levels of quantization, where it combines tensors and reduces down the scale (floating point precision) of some group thus yielding to reduction in model size. Read more: \u003Ca href=\"https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">here\u003C/a>\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cem>@oobabooga\u003C/em> (Founder of text-generation-webui) has published a detailed \u003Ca href=\"https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">report\u003C/a> on the comparison of different quantization techniques.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://i.imgur.com/Q0N9Rja.png\" alt=\"Model Perplexity\">\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">As you can see there is a significant reduction in memory as the model loses its perplexity.\u003C/p>\n\u003Ch2 id=\"llama.cpp\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">LLAMA.CPP\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#llama.cpp\">#\u003C/a>\u003C/h2>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Originally, this project emerged to execute LLAMA series models on CPU devices by compiling them into machine code specific to those devices. Over time, the project has expanded significantly to accommodate various model architectures, enabling execution on diverse hardware devices such as MAC, ARM, CUDA, and more. It is constructed with a focus on optimizing performance. Notably, the project has introduced a new format, GGML/GGUF, for storing and sharing the original model weights in a compressed format. Visualize it as a unified binary file encapsulating essential elements like tokenizers, models, configurations, vocabularies, etc., required for running your model—all neatly packed into a single format. GGUF succeeds GGML and extends its support to non-LLAMA models, incorporating token-extension capabilities. I highly recommend exploring this development further.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Project: \u003Ca href=\"https://github.com/ggerganov/llama.cpp\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">link\u003C/a>\u003C/p>\n\u003Ch2 id=\"what-are-my-options-for-small-language-models?\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">What are my options for small language models?\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#what-are-my-options-for-small-language-models?\">#\u003C/a>\u003C/h2>\n\u003Ch3 id=\"tinyllama-(1.1b)\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">TinyLlama (1.1B)\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#tinyllama-(1.1b)\">#\u003C/a>\u003C/h3>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">Same architecture and tokenizer as Llama 2 but parameters scaled down.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">Trained on 3 Trillion Tokens.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">Supports sequence lengths of 2048.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://github.com/jzhang38/TinyLlama\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Github\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"tinymistral-(248m)\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">TinyMistral (248M)\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#tinymistral-(248m)\">#\u003C/a>\u003C/h3>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">Based on Mistral architecture.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">The base model may not be suitable for conversation, but there are variants like the &quot;Instruct&quot; version designed to support chat applications.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">Supports sequence lengths of up to 32k.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://huggingface.co/Locutusque/TinyMistral-248M\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Github\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"smol-llama-(101m)\" class=\"group text-xl font-bold  mt-4 dark:text-neutral-200\">Smol Llama (101M)\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#smol-llama-(101m)\">#\u003C/a>\u003C/h3>\n\u003Cul class=\" space-y-1 list-disc list-inside\" >\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">A smaller version of the Llama model trained from scratch on different datasets.\u003C/li>\n\u003Cli class=\"text-neutral-700 dark:text-neutral-300 leading-loose mx-2 my-2 text-sm \">\u003Ca href=\"https://huggingface.co/Felladrin/Smol-Llama-101M-Chat-v1\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Github\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Ch1 id=\"llm.js\" class=\"group text-3xl font-bold mt-4 dark:text-neutral-200\">LLM.js\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#llm.js\">#\u003C/a>\u003C/h1>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Ca href=\"https://rahuldshetty.github.io/llm.js/\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">LLM.js\u003C/a> is my side hobby project, designed to facilitate the execution of Language Models in various inference formats on the web. Inspired by transformers.js, which aims to bring the transformer ecosystem into browsers, LLM.js is more specific, focusing on text-generation use cases with different language model formats and architectures.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The project owes its existence to the collaborative efforts of the community, particularly on projects like LLAMA.CPP, llama.c, and Emscripten. Emscripten enables the compilation of projects into the WebAssembly (WASM) format, allowing them to be executed in a web browser&#39;s WebAssembly engine.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://storage.googleapis.com/blog-images-backup/0*xNUJSNFWssBQcekQ.png\" alt=\"WASM Flow\">\u003C/p>\n\u003Ch2 id=\"llm.js---playground\" class=\"group text-2xl font-bold  mt-4 dark:text-neutral-200\">LLM.js - Playground\u003Ca class=\"mx-1 text-transparent group-hover:underline group-hover:underline-offset-8 group-hover:text-gray-700\" href=\"#llm.js---playground\">#\u003C/a>\u003C/h2>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Ca href=\"https://rahuldshetty.github.io/ggml.js-examples/playground.html\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Playground\u003C/a> simplifies the process of testing various Language Models (LM) directly in your web browser. No additional dependencies are required, apart from a standard web browser (which should support WASM, a feature most modern browsers include).\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">The playground allows you to effortlessly fetch supported models from Hugging Face and execute them in your browser. You can customize the initial prompt and engage in text generation. While currently, direct modification of model parameters is not supported, this functionality is actively under development for the playground UI.\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Playground Demo: \u003Ca href=\"https://rahuldshetty.github.io/ggml.js-examples/playground.html\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">link\u003C/a>\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">Source Code: \u003Ca href=\"https://github.com/rahuldshetty/ggml.js-examples/blob/master/playground.html\" class=\"underline underline-offset-8 md:underline-offset-4 text-sky-800 dark:text-sky-200\">Playground\u003C/a>\u003C/p>\n\u003Cp class=\"mt-4 text-neutral-700 dark:text-neutral-300 leading-loose\">\u003Cimg src=\"https://raw.githubusercontent.com/rahuldshetty/ggml.js-examples/master/llm-js.gif\" alt=\"LLM.js Playground\">\u003C/p>\n"}},"uses":{"params":["slug"]}}];

					Promise.all([
						import("../../_app/immutable/entry/start.92df42a1.js"),
						import("../../_app/immutable/entry/app.705885d4.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 4],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
